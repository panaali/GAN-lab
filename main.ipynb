{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"main.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"5s4EcwwZbnwc","colab_type":"text"},"cell_type":"markdown","source":["# GAN-Lab\n","The goal of this document is to test different GAN implementation with different parameters."]},{"metadata":{"colab_type":"text","id":"cGE-PzozwTbM"},"cell_type":"markdown","source":["## Instruction\n","* For running the code on GPU: Runtime > Change runtime type > Hardware Accelerator > GPU\n","* For running all the cells: âŒ˜/Ctrl + F9\n","\n","\n"]},{"metadata":{"id":"5LPO1zG-00I2","colab_type":"code","colab":{}},"cell_type":"code","source":["# Install Dependencies\n","try:\n","  import ipynb\n","except:\n","  !pip install git+https://github.com/panaali/ipynb.git"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"tfbdy0jrwTbG","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"96f455bb-74ec-4b1c-d855-558f2512ce39","executionInfo":{"status":"ok","timestamp":1544731437780,"user_tz":300,"elapsed":826,"user":{"displayName":"Ali Panahi","photoUrl":"https://lh5.googleusercontent.com/-aAEVDWwWDxU/AAAAAAAAAAI/AAAAAAAAEuM/WOpI1BZp8tM/s64/photo.jpg","userId":"09333706783536538061"}}},"cell_type":"code","source":["# User defined Variables\n","PROJECT_PATH_GDRIVE = 'Colab Projects/GAN-lab'\n","\n","# CONSTANTS\n","PROJECT_PATH_LOCAL = '/content/'\n","PROJECT_PATH_COLAB = '/content/drive/My Drive/' + PROJECT_PATH_GDRIVE\n","\n","# Mount Google Drive\n","from google.colab import drive\n","import os\n","\n","if not os.path.exists('/content/drive/'):\n","  drive.mount('/content/drive/')\n","\n","print('Project Mounted Path:')\n","%cd $PROJECT_PATH_COLAB"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Project Mounted Path:\n","/content/drive/My Drive/Colab Projects/GAN-lab\n"],"name":"stdout"}]},{"metadata":{"id":"PSh1Eox8xt1D","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":6018},"outputId":"15829417-b739-4ad8-ad99-eb1fc3734e52","executionInfo":{"status":"ok","timestamp":1544731457128,"user_tz":300,"elapsed":18349,"user":{"displayName":"Ali Panahi","photoUrl":"https://lh5.googleusercontent.com/-aAEVDWwWDxU/AAAAAAAAAAI/AAAAAAAAEuM/WOpI1BZp8tM/s64/photo.jpg","userId":"09333706783536538061"}}},"cell_type":"code","source":["import time\n","import shutil\n","import os\n","from ipynb.fs.full.Keras_GAN import GAN\n","\n","# Run Keras-GAN\n","start_time = time.time()\n","shutil.rmtree(PROJECT_PATH_LOCAL + './images', ignore_errors=True)\n","os.makedirs(PROJECT_PATH_LOCAL + './images', exist_ok=True)\n","\n","gan = GAN()\n","gan.train(epochs=301, batch_size=32, sample_interval=200)\n","print(\"--- %s seconds ---\" % (time.time() - start_time))"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"stream","text":["Discriminator Model:\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","flatten_1 (Flatten)          (None, 784)               0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 512)               401920    \n","_________________________________________________________________\n","leaky_re_lu_1 (LeakyReLU)    (None, 512)               0         \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 256)               131328    \n","_________________________________________________________________\n","leaky_re_lu_2 (LeakyReLU)    (None, 256)               0         \n","_________________________________________________________________\n","dense_3 (Dense)              (None, 1)                 257       \n","=================================================================\n","Total params: 533,505\n","Trainable params: 533,505\n","Non-trainable params: 0\n","_________________________________________________________________\n","Generator Model:\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense_4 (Dense)              (None, 256)               25856     \n","_________________________________________________________________\n","leaky_re_lu_3 (LeakyReLU)    (None, 256)               0         \n","_________________________________________________________________\n","batch_normalization_1 (Batch (None, 256)               1024      \n","_________________________________________________________________\n","dense_5 (Dense)              (None, 512)               131584    \n","_________________________________________________________________\n","leaky_re_lu_4 (LeakyReLU)    (None, 512)               0         \n","_________________________________________________________________\n","batch_normalization_2 (Batch (None, 512)               2048      \n","_________________________________________________________________\n","dense_6 (Dense)              (None, 1024)              525312    \n","_________________________________________________________________\n","leaky_re_lu_5 (LeakyReLU)    (None, 1024)              0         \n","_________________________________________________________________\n","batch_normalization_3 (Batch (None, 1024)              4096      \n","_________________________________________________________________\n","dense_7 (Dense)              (None, 784)               803600    \n","_________________________________________________________________\n","reshape_1 (Reshape)          (None, 28, 28, 1)         0         \n","=================================================================\n","Total params: 1,493,520\n","Trainable params: 1,489,936\n","Non-trainable params: 3,584\n","_________________________________________________________________\n","0 [D loss: 0.569732, acc.: 87.50%] [G loss: 0.907854]\n","1 [D loss: 0.351364, acc.: 84.38%] [G loss: 1.000545]\n","2 [D loss: 0.257363, acc.: 95.31%] [G loss: 1.173341]\n","3 [D loss: 0.223985, acc.: 95.31%] [G loss: 1.392742]\n","4 [D loss: 0.156323, acc.: 100.00%] [G loss: 1.437118]\n","5 [D loss: 0.125050, acc.: 100.00%] [G loss: 1.652990]\n","6 [D loss: 0.141040, acc.: 100.00%] [G loss: 1.820063]\n","7 [D loss: 0.092995, acc.: 100.00%] [G loss: 1.857915]\n","8 [D loss: 0.090888, acc.: 100.00%] [G loss: 2.041618]\n","9 [D loss: 0.089019, acc.: 100.00%] [G loss: 2.130236]\n","10 [D loss: 0.089207, acc.: 100.00%] [G loss: 2.200345]\n","11 [D loss: 0.074357, acc.: 100.00%] [G loss: 2.336824]\n","12 [D loss: 0.056132, acc.: 100.00%] [G loss: 2.372595]\n","13 [D loss: 0.072044, acc.: 100.00%] [G loss: 2.453867]\n","14 [D loss: 0.058169, acc.: 100.00%] [G loss: 2.508687]\n","15 [D loss: 0.048978, acc.: 100.00%] [G loss: 2.552210]\n","16 [D loss: 0.054008, acc.: 100.00%] [G loss: 2.642511]\n","17 [D loss: 0.039491, acc.: 100.00%] [G loss: 2.640128]\n","18 [D loss: 0.049180, acc.: 100.00%] [G loss: 2.919680]\n","19 [D loss: 0.042193, acc.: 100.00%] [G loss: 2.831104]\n","20 [D loss: 0.046241, acc.: 100.00%] [G loss: 2.820783]\n","21 [D loss: 0.043318, acc.: 100.00%] [G loss: 2.989602]\n","22 [D loss: 0.029852, acc.: 100.00%] [G loss: 2.990699]\n","23 [D loss: 0.036205, acc.: 100.00%] [G loss: 3.163559]\n","24 [D loss: 0.041248, acc.: 100.00%] [G loss: 3.138499]\n","25 [D loss: 0.033600, acc.: 100.00%] [G loss: 3.124425]\n","26 [D loss: 0.028681, acc.: 100.00%] [G loss: 3.312924]\n","27 [D loss: 0.028832, acc.: 100.00%] [G loss: 3.356268]\n","28 [D loss: 0.022953, acc.: 100.00%] [G loss: 3.363072]\n","29 [D loss: 0.024154, acc.: 100.00%] [G loss: 3.227604]\n","30 [D loss: 0.019647, acc.: 100.00%] [G loss: 3.289294]\n","31 [D loss: 0.020378, acc.: 100.00%] [G loss: 3.371649]\n","32 [D loss: 0.027786, acc.: 100.00%] [G loss: 3.384898]\n","33 [D loss: 0.030621, acc.: 100.00%] [G loss: 3.390168]\n","34 [D loss: 0.017102, acc.: 100.00%] [G loss: 3.519361]\n","35 [D loss: 0.025982, acc.: 100.00%] [G loss: 3.606411]\n","36 [D loss: 0.025392, acc.: 100.00%] [G loss: 3.803301]\n","37 [D loss: 0.017042, acc.: 100.00%] [G loss: 3.665531]\n","38 [D loss: 0.021062, acc.: 100.00%] [G loss: 3.829410]\n","39 [D loss: 0.017049, acc.: 100.00%] [G loss: 3.686924]\n","40 [D loss: 0.017750, acc.: 100.00%] [G loss: 3.826681]\n","41 [D loss: 0.019183, acc.: 100.00%] [G loss: 3.702193]\n","42 [D loss: 0.019513, acc.: 100.00%] [G loss: 4.018140]\n","43 [D loss: 0.014755, acc.: 100.00%] [G loss: 3.964573]\n","44 [D loss: 0.015411, acc.: 100.00%] [G loss: 4.004785]\n","45 [D loss: 0.015493, acc.: 100.00%] [G loss: 4.031127]\n","46 [D loss: 0.015084, acc.: 100.00%] [G loss: 4.130437]\n","47 [D loss: 0.016071, acc.: 100.00%] [G loss: 4.026956]\n","48 [D loss: 0.017217, acc.: 100.00%] [G loss: 3.946826]\n","49 [D loss: 0.012433, acc.: 100.00%] [G loss: 4.032289]\n","50 [D loss: 0.018444, acc.: 100.00%] [G loss: 4.097910]\n","51 [D loss: 0.017246, acc.: 100.00%] [G loss: 4.021109]\n","52 [D loss: 0.012605, acc.: 100.00%] [G loss: 3.982070]\n","53 [D loss: 0.011916, acc.: 100.00%] [G loss: 4.076950]\n","54 [D loss: 0.015135, acc.: 100.00%] [G loss: 4.227858]\n","55 [D loss: 0.010850, acc.: 100.00%] [G loss: 4.277536]\n","56 [D loss: 0.015273, acc.: 100.00%] [G loss: 4.151134]\n","57 [D loss: 0.014606, acc.: 100.00%] [G loss: 4.321423]\n","58 [D loss: 0.015718, acc.: 100.00%] [G loss: 4.184650]\n","59 [D loss: 0.014377, acc.: 100.00%] [G loss: 4.230031]\n","60 [D loss: 0.012411, acc.: 100.00%] [G loss: 4.317424]\n","61 [D loss: 0.014897, acc.: 100.00%] [G loss: 4.335691]\n","62 [D loss: 0.013209, acc.: 100.00%] [G loss: 4.490224]\n","63 [D loss: 0.012280, acc.: 100.00%] [G loss: 4.364329]\n","64 [D loss: 0.009119, acc.: 100.00%] [G loss: 4.292832]\n","65 [D loss: 0.013009, acc.: 100.00%] [G loss: 4.375514]\n","66 [D loss: 0.012174, acc.: 100.00%] [G loss: 4.285874]\n","67 [D loss: 0.010521, acc.: 100.00%] [G loss: 4.437897]\n","68 [D loss: 0.010599, acc.: 100.00%] [G loss: 4.442538]\n","69 [D loss: 0.018029, acc.: 100.00%] [G loss: 4.374827]\n","70 [D loss: 0.012643, acc.: 100.00%] [G loss: 4.682451]\n","71 [D loss: 0.019714, acc.: 100.00%] [G loss: 4.471452]\n","72 [D loss: 0.012340, acc.: 100.00%] [G loss: 4.440274]\n","73 [D loss: 0.014455, acc.: 100.00%] [G loss: 4.454560]\n","74 [D loss: 0.010593, acc.: 100.00%] [G loss: 4.526677]\n","75 [D loss: 0.012417, acc.: 100.00%] [G loss: 4.698005]\n","76 [D loss: 0.014081, acc.: 100.00%] [G loss: 4.528106]\n","77 [D loss: 0.010304, acc.: 100.00%] [G loss: 4.733712]\n","78 [D loss: 0.011968, acc.: 100.00%] [G loss: 4.595238]\n","79 [D loss: 0.012682, acc.: 100.00%] [G loss: 4.611562]\n","80 [D loss: 0.010024, acc.: 100.00%] [G loss: 4.643984]\n","81 [D loss: 0.015704, acc.: 100.00%] [G loss: 4.588591]\n","82 [D loss: 0.013237, acc.: 100.00%] [G loss: 4.778094]\n","83 [D loss: 0.014204, acc.: 100.00%] [G loss: 4.896995]\n","84 [D loss: 0.012037, acc.: 100.00%] [G loss: 4.815044]\n","85 [D loss: 0.015536, acc.: 100.00%] [G loss: 4.886545]\n","86 [D loss: 0.011453, acc.: 100.00%] [G loss: 4.674591]\n","87 [D loss: 0.007431, acc.: 100.00%] [G loss: 4.692529]\n","88 [D loss: 0.014790, acc.: 100.00%] [G loss: 4.645969]\n","89 [D loss: 0.017081, acc.: 100.00%] [G loss: 4.839069]\n","90 [D loss: 0.013128, acc.: 100.00%] [G loss: 4.810945]\n","91 [D loss: 0.013310, acc.: 100.00%] [G loss: 4.656833]\n","92 [D loss: 0.015393, acc.: 100.00%] [G loss: 4.651579]\n","93 [D loss: 0.010190, acc.: 100.00%] [G loss: 4.955348]\n","94 [D loss: 0.013601, acc.: 100.00%] [G loss: 4.858415]\n","95 [D loss: 0.008952, acc.: 100.00%] [G loss: 4.719229]\n","96 [D loss: 0.012093, acc.: 100.00%] [G loss: 4.692566]\n","97 [D loss: 0.012183, acc.: 100.00%] [G loss: 4.673639]\n","98 [D loss: 0.027835, acc.: 100.00%] [G loss: 5.038289]\n","99 [D loss: 0.025076, acc.: 100.00%] [G loss: 5.107532]\n","100 [D loss: 0.025660, acc.: 100.00%] [G loss: 4.947909]\n","101 [D loss: 0.012339, acc.: 100.00%] [G loss: 5.074451]\n","102 [D loss: 0.018501, acc.: 100.00%] [G loss: 4.979172]\n","103 [D loss: 0.020269, acc.: 100.00%] [G loss: 4.873081]\n","104 [D loss: 0.025570, acc.: 98.44%] [G loss: 5.400449]\n","105 [D loss: 0.063707, acc.: 98.44%] [G loss: 4.528384]\n","106 [D loss: 0.059381, acc.: 96.88%] [G loss: 5.431123]\n","107 [D loss: 0.150986, acc.: 95.31%] [G loss: 4.675271]\n","108 [D loss: 0.015678, acc.: 100.00%] [G loss: 5.497405]\n","109 [D loss: 0.017946, acc.: 100.00%] [G loss: 5.012625]\n","110 [D loss: 0.033913, acc.: 100.00%] [G loss: 5.120350]\n","111 [D loss: 0.021875, acc.: 100.00%] [G loss: 5.018810]\n","112 [D loss: 0.050177, acc.: 100.00%] [G loss: 5.218809]\n","113 [D loss: 0.063971, acc.: 96.88%] [G loss: 5.979114]\n","114 [D loss: 1.816460, acc.: 39.06%] [G loss: 5.125879]\n","115 [D loss: 1.434247, acc.: 68.75%] [G loss: 3.030587]\n","116 [D loss: 0.537072, acc.: 82.81%] [G loss: 2.655710]\n","117 [D loss: 0.560178, acc.: 78.12%] [G loss: 3.322330]\n","118 [D loss: 0.133676, acc.: 93.75%] [G loss: 3.415914]\n","119 [D loss: 0.175207, acc.: 95.31%] [G loss: 3.569803]\n","120 [D loss: 0.084939, acc.: 98.44%] [G loss: 3.268113]\n","121 [D loss: 0.054329, acc.: 100.00%] [G loss: 3.403430]\n","122 [D loss: 0.043109, acc.: 98.44%] [G loss: 3.360137]\n","123 [D loss: 0.060770, acc.: 100.00%] [G loss: 3.695015]\n","124 [D loss: 0.049223, acc.: 100.00%] [G loss: 3.740158]\n","125 [D loss: 0.067901, acc.: 100.00%] [G loss: 3.675550]\n","126 [D loss: 0.045637, acc.: 100.00%] [G loss: 3.565452]\n","127 [D loss: 0.048287, acc.: 100.00%] [G loss: 3.492429]\n","128 [D loss: 0.070153, acc.: 98.44%] [G loss: 3.166003]\n","129 [D loss: 0.067876, acc.: 98.44%] [G loss: 3.292911]\n","130 [D loss: 0.073258, acc.: 98.44%] [G loss: 3.266414]\n","131 [D loss: 0.075018, acc.: 100.00%] [G loss: 3.111460]\n","132 [D loss: 0.099564, acc.: 96.88%] [G loss: 3.356130]\n","133 [D loss: 0.058294, acc.: 98.44%] [G loss: 3.243936]\n","134 [D loss: 0.081181, acc.: 100.00%] [G loss: 3.418984]\n","135 [D loss: 0.067545, acc.: 100.00%] [G loss: 3.374951]\n","136 [D loss: 0.141710, acc.: 93.75%] [G loss: 3.405962]\n","137 [D loss: 0.084012, acc.: 98.44%] [G loss: 3.376809]\n","138 [D loss: 0.127015, acc.: 96.88%] [G loss: 3.514373]\n","139 [D loss: 0.102362, acc.: 100.00%] [G loss: 3.117923]\n","140 [D loss: 0.118729, acc.: 98.44%] [G loss: 3.063303]\n","141 [D loss: 0.082221, acc.: 100.00%] [G loss: 2.977072]\n","142 [D loss: 0.146666, acc.: 95.31%] [G loss: 3.230306]\n","143 [D loss: 0.206550, acc.: 92.19%] [G loss: 3.170949]\n","144 [D loss: 0.204499, acc.: 90.62%] [G loss: 3.091298]\n","145 [D loss: 0.095293, acc.: 96.88%] [G loss: 3.529939]\n","146 [D loss: 0.085038, acc.: 100.00%] [G loss: 3.098909]\n","147 [D loss: 0.157110, acc.: 95.31%] [G loss: 3.213850]\n","148 [D loss: 0.159028, acc.: 95.31%] [G loss: 3.266787]\n","149 [D loss: 0.168269, acc.: 95.31%] [G loss: 3.259543]\n","150 [D loss: 0.122413, acc.: 96.88%] [G loss: 3.446462]\n","151 [D loss: 0.134319, acc.: 96.88%] [G loss: 3.461457]\n","152 [D loss: 0.142909, acc.: 95.31%] [G loss: 3.194081]\n","153 [D loss: 0.134726, acc.: 96.88%] [G loss: 3.490113]\n","154 [D loss: 0.344892, acc.: 84.38%] [G loss: 2.949962]\n","155 [D loss: 0.129297, acc.: 95.31%] [G loss: 4.149990]\n","156 [D loss: 0.149445, acc.: 96.88%] [G loss: 3.678830]\n","157 [D loss: 0.131775, acc.: 95.31%] [G loss: 3.633746]\n","158 [D loss: 0.071242, acc.: 100.00%] [G loss: 3.655860]\n","159 [D loss: 0.139340, acc.: 95.31%] [G loss: 3.597103]\n","160 [D loss: 0.117608, acc.: 96.88%] [G loss: 4.002429]\n","161 [D loss: 0.330487, acc.: 85.94%] [G loss: 2.681484]\n","162 [D loss: 0.151909, acc.: 93.75%] [G loss: 4.209542]\n","163 [D loss: 0.615232, acc.: 75.00%] [G loss: 2.672972]\n","164 [D loss: 0.093860, acc.: 98.44%] [G loss: 3.735732]\n","165 [D loss: 0.147357, acc.: 96.88%] [G loss: 3.770131]\n","166 [D loss: 0.313343, acc.: 84.38%] [G loss: 3.686992]\n","167 [D loss: 0.128074, acc.: 98.44%] [G loss: 3.395007]\n","168 [D loss: 0.106732, acc.: 96.88%] [G loss: 3.462881]\n","169 [D loss: 0.177707, acc.: 93.75%] [G loss: 3.252909]\n","170 [D loss: 0.175479, acc.: 93.75%] [G loss: 4.034142]\n","171 [D loss: 0.409931, acc.: 79.69%] [G loss: 2.971503]\n","172 [D loss: 0.125781, acc.: 95.31%] [G loss: 4.533331]\n","173 [D loss: 1.086718, acc.: 56.25%] [G loss: 1.698010]\n","174 [D loss: 0.331154, acc.: 76.56%] [G loss: 3.106699]\n","175 [D loss: 0.075499, acc.: 98.44%] [G loss: 3.810857]\n","176 [D loss: 0.144091, acc.: 96.88%] [G loss: 3.371117]\n","177 [D loss: 0.180870, acc.: 89.06%] [G loss: 4.140135]\n","178 [D loss: 0.324905, acc.: 84.38%] [G loss: 3.885901]\n","179 [D loss: 0.139373, acc.: 92.19%] [G loss: 3.861168]\n","180 [D loss: 0.143319, acc.: 96.88%] [G loss: 3.425787]\n","181 [D loss: 0.277160, acc.: 87.50%] [G loss: 3.282429]\n","182 [D loss: 0.121189, acc.: 96.88%] [G loss: 3.534322]\n","183 [D loss: 0.169638, acc.: 96.88%] [G loss: 3.201397]\n","184 [D loss: 0.288474, acc.: 84.38%] [G loss: 3.748220]\n","185 [D loss: 0.653836, acc.: 73.44%] [G loss: 2.850405]\n","186 [D loss: 0.097995, acc.: 96.88%] [G loss: 4.058182]\n","187 [D loss: 1.196691, acc.: 51.56%] [G loss: 1.681874]\n","188 [D loss: 0.303392, acc.: 81.25%] [G loss: 3.488053]\n","189 [D loss: 0.101016, acc.: 100.00%] [G loss: 4.204409]\n","190 [D loss: 0.350554, acc.: 82.81%] [G loss: 3.259809]\n","191 [D loss: 0.106906, acc.: 100.00%] [G loss: 3.699750]\n","192 [D loss: 0.412179, acc.: 79.69%] [G loss: 2.805047]\n","193 [D loss: 0.107431, acc.: 100.00%] [G loss: 3.301771]\n","194 [D loss: 0.333625, acc.: 89.06%] [G loss: 2.991861]\n","195 [D loss: 0.204933, acc.: 95.31%] [G loss: 3.735228]\n","196 [D loss: 0.578019, acc.: 75.00%] [G loss: 2.486084]\n","197 [D loss: 0.183802, acc.: 95.31%] [G loss: 3.849526]\n","198 [D loss: 0.567580, acc.: 65.62%] [G loss: 2.685792]\n","199 [D loss: 0.221374, acc.: 90.62%] [G loss: 3.666115]\n","200 [D loss: 0.133047, acc.: 95.31%] [G loss: 3.597485]\n","201 [D loss: 0.208003, acc.: 92.19%] [G loss: 3.109665]\n","202 [D loss: 0.193582, acc.: 96.88%] [G loss: 3.752459]\n","203 [D loss: 0.514308, acc.: 76.56%] [G loss: 4.140189]\n","204 [D loss: 0.285328, acc.: 89.06%] [G loss: 2.543311]\n","205 [D loss: 0.175951, acc.: 95.31%] [G loss: 3.205350]\n","206 [D loss: 0.339101, acc.: 84.38%] [G loss: 3.420900]\n","207 [D loss: 0.536244, acc.: 70.31%] [G loss: 4.363632]\n","208 [D loss: 1.206452, acc.: 40.62%] [G loss: 1.468225]\n","209 [D loss: 0.531784, acc.: 73.44%] [G loss: 1.771264]\n","210 [D loss: 0.205348, acc.: 90.62%] [G loss: 4.080562]\n","211 [D loss: 0.087116, acc.: 100.00%] [G loss: 4.606628]\n","212 [D loss: 0.165508, acc.: 98.44%] [G loss: 3.744311]\n","213 [D loss: 0.185388, acc.: 90.62%] [G loss: 3.585927]\n","214 [D loss: 0.386666, acc.: 78.12%] [G loss: 3.439655]\n","215 [D loss: 0.290601, acc.: 92.19%] [G loss: 2.695310]\n","216 [D loss: 0.180144, acc.: 93.75%] [G loss: 3.059075]\n","217 [D loss: 0.265224, acc.: 90.62%] [G loss: 3.568605]\n","218 [D loss: 0.612529, acc.: 65.62%] [G loss: 2.696800]\n","219 [D loss: 0.178611, acc.: 93.75%] [G loss: 3.175935]\n","220 [D loss: 0.320017, acc.: 85.94%] [G loss: 2.429331]\n","221 [D loss: 0.187704, acc.: 92.19%] [G loss: 2.965662]\n","222 [D loss: 0.311398, acc.: 90.62%] [G loss: 2.724371]\n","223 [D loss: 0.275113, acc.: 90.62%] [G loss: 3.181648]\n","224 [D loss: 0.545991, acc.: 70.31%] [G loss: 2.815687]\n","225 [D loss: 0.296071, acc.: 87.50%] [G loss: 2.937305]\n","226 [D loss: 0.567368, acc.: 68.75%] [G loss: 2.145573]\n","227 [D loss: 0.250359, acc.: 92.19%] [G loss: 4.022423]\n","228 [D loss: 1.075064, acc.: 39.06%] [G loss: 1.357903]\n","229 [D loss: 0.433095, acc.: 73.44%] [G loss: 3.309489]\n","230 [D loss: 0.160992, acc.: 100.00%] [G loss: 3.965903]\n","231 [D loss: 0.947337, acc.: 48.44%] [G loss: 1.800578]\n","232 [D loss: 0.238924, acc.: 90.62%] [G loss: 2.941741]\n","233 [D loss: 0.569936, acc.: 68.75%] [G loss: 2.347619]\n","234 [D loss: 0.485056, acc.: 68.75%] [G loss: 3.545687]\n","235 [D loss: 1.014211, acc.: 40.62%] [G loss: 1.307944]\n","236 [D loss: 0.354092, acc.: 78.12%] [G loss: 2.647709]\n","237 [D loss: 0.330085, acc.: 89.06%] [G loss: 2.258879]\n","238 [D loss: 0.399097, acc.: 87.50%] [G loss: 2.453506]\n","239 [D loss: 0.470514, acc.: 71.88%] [G loss: 2.665508]\n","240 [D loss: 0.504925, acc.: 75.00%] [G loss: 2.493962]\n","241 [D loss: 0.518991, acc.: 70.31%] [G loss: 2.365876]\n","242 [D loss: 0.440044, acc.: 73.44%] [G loss: 2.639455]\n","243 [D loss: 0.619346, acc.: 70.31%] [G loss: 1.819341]\n","244 [D loss: 0.373403, acc.: 82.81%] [G loss: 2.485219]\n","245 [D loss: 0.582745, acc.: 71.88%] [G loss: 2.012866]\n","246 [D loss: 0.554265, acc.: 67.19%] [G loss: 2.498783]\n","247 [D loss: 0.781485, acc.: 46.88%] [G loss: 1.609343]\n","248 [D loss: 0.439619, acc.: 81.25%] [G loss: 2.692443]\n","249 [D loss: 0.996290, acc.: 40.62%] [G loss: 0.828906]\n","250 [D loss: 0.371398, acc.: 81.25%] [G loss: 2.066796]\n","251 [D loss: 0.287456, acc.: 93.75%] [G loss: 2.874614]\n","252 [D loss: 0.904389, acc.: 43.75%] [G loss: 1.319587]\n","253 [D loss: 0.394470, acc.: 76.56%] [G loss: 2.195145]\n","254 [D loss: 0.889920, acc.: 42.19%] [G loss: 1.198126]\n","255 [D loss: 0.432708, acc.: 75.00%] [G loss: 2.305073]\n","256 [D loss: 0.867979, acc.: 45.31%] [G loss: 1.264033]\n","257 [D loss: 0.617461, acc.: 56.25%] [G loss: 2.076930]\n","258 [D loss: 0.825523, acc.: 48.44%] [G loss: 1.157802]\n","259 [D loss: 0.555401, acc.: 68.75%] [G loss: 1.743497]\n","260 [D loss: 0.654195, acc.: 64.06%] [G loss: 1.789245]\n","261 [D loss: 0.720590, acc.: 57.81%] [G loss: 1.390419]\n","262 [D loss: 0.648129, acc.: 62.50%] [G loss: 1.514349]\n","263 [D loss: 0.898846, acc.: 40.62%] [G loss: 1.009434]\n","264 [D loss: 0.549813, acc.: 60.94%] [G loss: 2.077668]\n","265 [D loss: 1.022244, acc.: 29.69%] [G loss: 0.904060]\n","266 [D loss: 0.583482, acc.: 64.06%] [G loss: 1.155460]\n","267 [D loss: 0.538880, acc.: 73.44%] [G loss: 1.660037]\n","268 [D loss: 0.760820, acc.: 46.88%] [G loss: 1.158571]\n","269 [D loss: 0.607323, acc.: 59.38%] [G loss: 1.326475]\n","270 [D loss: 0.704132, acc.: 46.88%] [G loss: 1.341904]\n","271 [D loss: 0.879006, acc.: 35.94%] [G loss: 0.922518]\n","272 [D loss: 0.628320, acc.: 60.94%] [G loss: 1.190341]\n","273 [D loss: 0.658042, acc.: 48.44%] [G loss: 1.208919]\n","274 [D loss: 0.839254, acc.: 39.06%] [G loss: 0.885501]\n","275 [D loss: 0.664794, acc.: 50.00%] [G loss: 1.149242]\n","276 [D loss: 0.835789, acc.: 37.50%] [G loss: 0.943390]\n","277 [D loss: 0.702704, acc.: 46.88%] [G loss: 1.207891]\n","278 [D loss: 0.812060, acc.: 42.19%] [G loss: 0.925743]\n","279 [D loss: 0.699812, acc.: 45.31%] [G loss: 0.950192]\n","280 [D loss: 0.755459, acc.: 50.00%] [G loss: 0.939380]\n","281 [D loss: 0.754704, acc.: 46.88%] [G loss: 0.983653]\n","282 [D loss: 0.599076, acc.: 59.38%] [G loss: 1.167721]\n","283 [D loss: 0.703743, acc.: 51.56%] [G loss: 0.970240]\n","284 [D loss: 0.789509, acc.: 39.06%] [G loss: 0.899878]\n","285 [D loss: 0.722580, acc.: 51.56%] [G loss: 0.943086]\n","286 [D loss: 0.604460, acc.: 60.94%] [G loss: 1.142460]\n","287 [D loss: 0.878376, acc.: 31.25%] [G loss: 0.791063]\n","288 [D loss: 0.650913, acc.: 51.56%] [G loss: 0.963017]\n","289 [D loss: 0.774877, acc.: 37.50%] [G loss: 0.847757]\n","290 [D loss: 0.781097, acc.: 43.75%] [G loss: 0.742623]\n","291 [D loss: 0.748372, acc.: 42.19%] [G loss: 0.799167]\n","292 [D loss: 0.773478, acc.: 32.81%] [G loss: 0.844164]\n","293 [D loss: 0.645919, acc.: 53.12%] [G loss: 0.936363]\n","294 [D loss: 0.809334, acc.: 32.81%] [G loss: 0.715748]\n","295 [D loss: 0.791403, acc.: 35.94%] [G loss: 0.722511]\n","296 [D loss: 0.702892, acc.: 45.31%] [G loss: 0.831981]\n","297 [D loss: 0.782633, acc.: 35.94%] [G loss: 0.740055]\n","298 [D loss: 0.754829, acc.: 43.75%] [G loss: 0.852209]\n","299 [D loss: 0.839078, acc.: 29.69%] [G loss: 0.689131]\n","300 [D loss: 0.797296, acc.: 42.19%] [G loss: 0.730361]\n","--- 16.704500913619995 seconds ---\n"],"name":"stdout"}]},{"metadata":{"id":"_jdmVRjoyC1z","colab_type":"code","colab":{}},"cell_type":"code","source":["# Sample live plot\n","import time\n","import pylab as pl\n","from IPython import display\n","for i in range(3):\n","    pl.cla()\n","    pl.plot(pl.randn(100))\n","    display.clear_output(wait=False)\n","    display.display(pl.gcf())\n","    time.sleep(1.0)\n","display.clear_output(wait=True)"],"execution_count":0,"outputs":[]}]}