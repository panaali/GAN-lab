{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"main.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"5s4EcwwZbnwc","colab_type":"text"},"cell_type":"markdown","source":["# GAN-Lab\n","The goal of this document is to test different GAN implementation with different parameters."]},{"metadata":{"colab_type":"text","id":"cGE-PzozwTbM"},"cell_type":"markdown","source":["## Instruction\n","* For running the code on GPU: Runtime > Change runtime type > Hardware Accelerator > GPU\n","* For running all the cells: âŒ˜/Ctrl + F9\n","\n","\n"]},{"metadata":{"id":"5LPO1zG-00I2","colab_type":"code","outputId":"afe1628d-a126-42d7-f3e7-a6eced054cc3","executionInfo":{"status":"ok","timestamp":1545427766956,"user_tz":300,"elapsed":5528,"user":{"displayName":"Ali Panahi","photoUrl":"https://lh5.googleusercontent.com/-aAEVDWwWDxU/AAAAAAAAAAI/AAAAAAAAEuM/WOpI1BZp8tM/s64/photo.jpg","userId":"09333706783536538061"}},"colab":{"base_uri":"https://localhost:8080/","height":224}},"cell_type":"code","source":["# Install Dependencies\n","try:\n","  import ipynb\n","except:\n","  !pip install git+https://github.com/panaali/ipynb.git\n","\n","try:\n","  import tensorboardcolab\n","except:\n","  !pip install -U tensorboardcolab"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting git+https://github.com/panaali/ipynb.git\r\n","  Cloning https://github.com/panaali/ipynb.git to /private/var/folders/px/6yb_jy3d23s3bvbjqxkfs0p80000gn/T/pip-req-build-B5F94S\n","\u001b[31mipynb requires Python '>=3.4' but the running Python is 2.7.15\u001b[0m\n","Collecting tensorboardcolab\n","  Downloading https://files.pythonhosted.org/packages/d9/28/97bf50473dc058d26188ef3aae373e56173d24c615fb419705cfffa6875d/tensorboardcolab-0.0.22.tar.gz\n","Building wheels for collected packages: tensorboardcolab\n","  Running setup.py bdist_wheel for tensorboardcolab ... \u001b[?25l-\b \bdone\n","\u001b[?25h  Stored in directory: /Users/aliakbarpanahi/Library/Caches/pip/wheels/c4/aa/a0/3aaf4f1a66adbdab9b7bdd4c96db8ada89eb7cd87200cfdd32\n","Successfully built tensorboardcolab\n","Installing collected packages: tensorboardcolab\n","Successfully installed tensorboardcolab-0.0.22\n"],"name":"stdout"}]},{"metadata":{"colab_type":"code","id":"tfbdy0jrwTbG","outputId":"0be98b13-f7e7-4213-d712-0d8b0b20e771","executionInfo":{"status":"ok","timestamp":1544753890207,"user_tz":300,"elapsed":1226,"user":{"displayName":"Ali Panahi","photoUrl":"https://lh5.googleusercontent.com/-aAEVDWwWDxU/AAAAAAAAAAI/AAAAAAAAEuM/WOpI1BZp8tM/s64/photo.jpg","userId":"09333706783536538061"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"cell_type":"code","source":["# User defined CONSTANTS\n","PROJECT_PATH_GDRIVE = 'Colab Projects/GAN-lab/'\n","IMAGE_PATH = PROJECT_PATH_LOCAL + './images/'\n","\n","# Pre-defined CONSTANTS\n","PROJECT_PATH_LOCAL = '/content/'\n","PROJECT_PATH_COLAB = '/content/drive/My Drive/' + PROJECT_PATH_GDRIVE\n","\n","# Mount Google Drive\n","from google.colab import drive\n","import os\n","\n","if not os.path.exists('/content/drive/'):\n","  drive.mount('/content/drive/')\n","\n","print('Project Mounted Path:')\n","%cd $PROJECT_PATH_COLAB"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Project Mounted Path:\n","/content/drive/My Drive/Colab Projects/GAN-lab\n"],"name":"stdout"}]},{"metadata":{"id":"PSh1Eox8xt1D","colab_type":"code","outputId":"cc272a10-b0c9-41d1-b76a-0fab42c82c9f","executionInfo":{"status":"ok","timestamp":1544733629258,"user_tz":300,"elapsed":22016,"user":{"displayName":"Ali Panahi","photoUrl":"https://lh5.googleusercontent.com/-aAEVDWwWDxU/AAAAAAAAAAI/AAAAAAAAEuM/WOpI1BZp8tM/s64/photo.jpg","userId":"09333706783536538061"}},"colab":{"base_uri":"https://localhost:8080/","height":7701}},"cell_type":"code","source":["import time\n","import shutil\n","import os\n","import importlib\n","\n","import ipynb.fs.full.Keras_GAN\n","importlib.reload(ipynb.fs.full.Keras_GAN)\n","from ipynb.fs.full.Keras_GAN import GAN\n","\n","# Run Keras-GAN\n","start_time = time.time()\n","\n","shutil.rmtree(PROJECT_PATH_LOCAL + './images', ignore_errors=True)\n","tf.gfile.MakeDirs(PROJECT_PATH_LOCAL + './images', exist_ok=True)\n","\n","gan = GAN(IMAGE_PATH)\n","gan.train(epochs=401, batch_size=32, sample_interval=200)\n","print(\"--- %s seconds ---\" % (time.time() - start_time))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Discriminator Model:\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","flatten_3 (Flatten)          (None, 784)               0         \n","_________________________________________________________________\n","dense_15 (Dense)             (None, 512)               401920    \n","_________________________________________________________________\n","leaky_re_lu_11 (LeakyReLU)   (None, 512)               0         \n","_________________________________________________________________\n","dense_16 (Dense)             (None, 256)               131328    \n","_________________________________________________________________\n","leaky_re_lu_12 (LeakyReLU)   (None, 256)               0         \n","_________________________________________________________________\n","dense_17 (Dense)             (None, 1)                 257       \n","=================================================================\n","Total params: 533,505\n","Trainable params: 533,505\n","Non-trainable params: 0\n","_________________________________________________________________\n","Generator Model:\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense_18 (Dense)             (None, 256)               25856     \n","_________________________________________________________________\n","leaky_re_lu_13 (LeakyReLU)   (None, 256)               0         \n","_________________________________________________________________\n","batch_normalization_7 (Batch (None, 256)               1024      \n","_________________________________________________________________\n","dense_19 (Dense)             (None, 512)               131584    \n","_________________________________________________________________\n","leaky_re_lu_14 (LeakyReLU)   (None, 512)               0         \n","_________________________________________________________________\n","batch_normalization_8 (Batch (None, 512)               2048      \n","_________________________________________________________________\n","dense_20 (Dense)             (None, 1024)              525312    \n","_________________________________________________________________\n","leaky_re_lu_15 (LeakyReLU)   (None, 1024)              0         \n","_________________________________________________________________\n","batch_normalization_9 (Batch (None, 1024)              4096      \n","_________________________________________________________________\n","dense_21 (Dense)             (None, 784)               803600    \n","_________________________________________________________________\n","reshape_3 (Reshape)          (None, 28, 28, 1)         0         \n","=================================================================\n","Total params: 1,493,520\n","Trainable params: 1,489,936\n","Non-trainable params: 3,584\n","_________________________________________________________________\n","0 [D loss: 0.779337, acc.: 26.56%] [G loss: 0.642057]\n","1 [D loss: 0.418695, acc.: 71.88%] [G loss: 0.694718]\n","2 [D loss: 0.336677, acc.: 79.69%] [G loss: 0.820349]\n","3 [D loss: 0.299393, acc.: 87.50%] [G loss: 0.992347]\n","4 [D loss: 0.231190, acc.: 93.75%] [G loss: 1.139121]\n","5 [D loss: 0.222513, acc.: 98.44%] [G loss: 1.288606]\n","6 [D loss: 0.140074, acc.: 100.00%] [G loss: 1.469736]\n","7 [D loss: 0.199081, acc.: 100.00%] [G loss: 1.566015]\n","8 [D loss: 0.159537, acc.: 98.44%] [G loss: 1.649272]\n","9 [D loss: 0.139318, acc.: 100.00%] [G loss: 1.765327]\n","10 [D loss: 0.117014, acc.: 100.00%] [G loss: 1.957238]\n","11 [D loss: 0.095336, acc.: 100.00%] [G loss: 1.962172]\n","12 [D loss: 0.095173, acc.: 100.00%] [G loss: 2.079451]\n","13 [D loss: 0.085198, acc.: 100.00%] [G loss: 2.132257]\n","14 [D loss: 0.083007, acc.: 100.00%] [G loss: 2.257463]\n","15 [D loss: 0.079886, acc.: 100.00%] [G loss: 2.241645]\n","16 [D loss: 0.068251, acc.: 100.00%] [G loss: 2.431020]\n","17 [D loss: 0.068931, acc.: 100.00%] [G loss: 2.318368]\n","18 [D loss: 0.073510, acc.: 100.00%] [G loss: 2.492713]\n","19 [D loss: 0.060022, acc.: 100.00%] [G loss: 2.594047]\n","20 [D loss: 0.050351, acc.: 100.00%] [G loss: 2.575635]\n","21 [D loss: 0.043724, acc.: 100.00%] [G loss: 2.641345]\n","22 [D loss: 0.044027, acc.: 100.00%] [G loss: 2.779525]\n","23 [D loss: 0.042774, acc.: 100.00%] [G loss: 2.697606]\n","24 [D loss: 0.041897, acc.: 100.00%] [G loss: 2.794539]\n","25 [D loss: 0.034636, acc.: 100.00%] [G loss: 2.806753]\n","26 [D loss: 0.042873, acc.: 100.00%] [G loss: 2.808252]\n","27 [D loss: 0.030819, acc.: 100.00%] [G loss: 2.897702]\n","28 [D loss: 0.033313, acc.: 100.00%] [G loss: 2.982521]\n","29 [D loss: 0.033779, acc.: 100.00%] [G loss: 2.896269]\n","30 [D loss: 0.039215, acc.: 100.00%] [G loss: 2.993516]\n","31 [D loss: 0.047058, acc.: 100.00%] [G loss: 3.125560]\n","32 [D loss: 0.026633, acc.: 100.00%] [G loss: 3.198521]\n","33 [D loss: 0.028224, acc.: 100.00%] [G loss: 3.212743]\n","34 [D loss: 0.029854, acc.: 100.00%] [G loss: 3.275175]\n","35 [D loss: 0.029921, acc.: 100.00%] [G loss: 3.300254]\n","36 [D loss: 0.023482, acc.: 100.00%] [G loss: 3.283600]\n","37 [D loss: 0.029586, acc.: 100.00%] [G loss: 3.271068]\n","38 [D loss: 0.026266, acc.: 100.00%] [G loss: 3.473553]\n","39 [D loss: 0.026011, acc.: 100.00%] [G loss: 3.512806]\n","40 [D loss: 0.022748, acc.: 100.00%] [G loss: 3.468780]\n","41 [D loss: 0.019730, acc.: 100.00%] [G loss: 3.663193]\n","42 [D loss: 0.019715, acc.: 100.00%] [G loss: 3.476942]\n","43 [D loss: 0.020627, acc.: 100.00%] [G loss: 3.505623]\n","44 [D loss: 0.017922, acc.: 100.00%] [G loss: 3.679076]\n","45 [D loss: 0.025042, acc.: 100.00%] [G loss: 3.562532]\n","46 [D loss: 0.023522, acc.: 100.00%] [G loss: 3.686232]\n","47 [D loss: 0.024179, acc.: 100.00%] [G loss: 3.769845]\n","48 [D loss: 0.021414, acc.: 100.00%] [G loss: 3.812864]\n","49 [D loss: 0.014757, acc.: 100.00%] [G loss: 3.681784]\n","50 [D loss: 0.021347, acc.: 100.00%] [G loss: 3.661852]\n","51 [D loss: 0.018415, acc.: 100.00%] [G loss: 3.777895]\n","52 [D loss: 0.017984, acc.: 100.00%] [G loss: 3.892017]\n","53 [D loss: 0.017154, acc.: 100.00%] [G loss: 4.002898]\n","54 [D loss: 0.018159, acc.: 100.00%] [G loss: 3.787142]\n","55 [D loss: 0.012435, acc.: 100.00%] [G loss: 3.700644]\n","56 [D loss: 0.015896, acc.: 100.00%] [G loss: 3.788334]\n","57 [D loss: 0.018493, acc.: 100.00%] [G loss: 3.953192]\n","58 [D loss: 0.014369, acc.: 100.00%] [G loss: 3.911366]\n","59 [D loss: 0.015855, acc.: 100.00%] [G loss: 3.904881]\n","60 [D loss: 0.019766, acc.: 100.00%] [G loss: 3.946778]\n","61 [D loss: 0.015269, acc.: 100.00%] [G loss: 4.012308]\n","62 [D loss: 0.021158, acc.: 100.00%] [G loss: 3.971846]\n","63 [D loss: 0.012144, acc.: 100.00%] [G loss: 4.057760]\n","64 [D loss: 0.013095, acc.: 100.00%] [G loss: 3.818793]\n","65 [D loss: 0.020588, acc.: 100.00%] [G loss: 3.978848]\n","66 [D loss: 0.015909, acc.: 100.00%] [G loss: 3.959782]\n","67 [D loss: 0.019241, acc.: 100.00%] [G loss: 4.092562]\n","68 [D loss: 0.013651, acc.: 100.00%] [G loss: 4.005661]\n","69 [D loss: 0.017552, acc.: 100.00%] [G loss: 4.133214]\n","70 [D loss: 0.012676, acc.: 100.00%] [G loss: 4.138578]\n","71 [D loss: 0.013973, acc.: 100.00%] [G loss: 4.234963]\n","72 [D loss: 0.017294, acc.: 100.00%] [G loss: 4.208916]\n","73 [D loss: 0.016441, acc.: 100.00%] [G loss: 4.285886]\n","74 [D loss: 0.013440, acc.: 100.00%] [G loss: 4.184461]\n","75 [D loss: 0.018401, acc.: 100.00%] [G loss: 4.165746]\n","76 [D loss: 0.012026, acc.: 100.00%] [G loss: 4.313975]\n","77 [D loss: 0.011525, acc.: 100.00%] [G loss: 4.260148]\n","78 [D loss: 0.010780, acc.: 100.00%] [G loss: 4.168741]\n","79 [D loss: 0.018430, acc.: 100.00%] [G loss: 4.123019]\n","80 [D loss: 0.016379, acc.: 100.00%] [G loss: 4.227960]\n","81 [D loss: 0.009868, acc.: 100.00%] [G loss: 4.363913]\n","82 [D loss: 0.015264, acc.: 100.00%] [G loss: 4.134981]\n","83 [D loss: 0.014882, acc.: 100.00%] [G loss: 4.227632]\n","84 [D loss: 0.014187, acc.: 100.00%] [G loss: 4.467318]\n","85 [D loss: 0.013558, acc.: 100.00%] [G loss: 4.277927]\n","86 [D loss: 0.013450, acc.: 100.00%] [G loss: 4.252808]\n","87 [D loss: 0.011907, acc.: 100.00%] [G loss: 4.427444]\n","88 [D loss: 0.014386, acc.: 100.00%] [G loss: 4.307785]\n","89 [D loss: 0.020139, acc.: 100.00%] [G loss: 4.471375]\n","90 [D loss: 0.014499, acc.: 100.00%] [G loss: 4.391486]\n","91 [D loss: 0.031540, acc.: 100.00%] [G loss: 4.758295]\n","92 [D loss: 0.016827, acc.: 100.00%] [G loss: 4.818499]\n","93 [D loss: 0.015539, acc.: 100.00%] [G loss: 4.509925]\n","94 [D loss: 0.019245, acc.: 100.00%] [G loss: 4.692632]\n","95 [D loss: 0.013266, acc.: 100.00%] [G loss: 4.686264]\n","96 [D loss: 0.012587, acc.: 100.00%] [G loss: 4.844738]\n","97 [D loss: 0.009720, acc.: 100.00%] [G loss: 4.722078]\n","98 [D loss: 0.015916, acc.: 100.00%] [G loss: 4.748553]\n","99 [D loss: 0.012311, acc.: 100.00%] [G loss: 4.701737]\n","100 [D loss: 0.011082, acc.: 100.00%] [G loss: 4.734119]\n","101 [D loss: 0.011504, acc.: 100.00%] [G loss: 4.739169]\n","102 [D loss: 0.019296, acc.: 100.00%] [G loss: 4.822886]\n","103 [D loss: 0.017617, acc.: 100.00%] [G loss: 4.838762]\n","104 [D loss: 0.013682, acc.: 100.00%] [G loss: 4.829173]\n","105 [D loss: 0.010918, acc.: 100.00%] [G loss: 4.757643]\n","106 [D loss: 0.010612, acc.: 100.00%] [G loss: 4.661361]\n","107 [D loss: 0.013389, acc.: 100.00%] [G loss: 4.683840]\n","108 [D loss: 0.014545, acc.: 100.00%] [G loss: 4.615510]\n","109 [D loss: 0.016061, acc.: 100.00%] [G loss: 4.713613]\n","110 [D loss: 0.011696, acc.: 100.00%] [G loss: 4.691225]\n","111 [D loss: 0.017440, acc.: 100.00%] [G loss: 4.775541]\n","112 [D loss: 0.013014, acc.: 100.00%] [G loss: 4.846947]\n","113 [D loss: 0.017965, acc.: 100.00%] [G loss: 4.853767]\n","114 [D loss: 0.018127, acc.: 100.00%] [G loss: 4.703721]\n","115 [D loss: 0.014809, acc.: 100.00%] [G loss: 4.544064]\n","116 [D loss: 0.021873, acc.: 100.00%] [G loss: 4.954288]\n","117 [D loss: 0.013236, acc.: 100.00%] [G loss: 4.862385]\n","118 [D loss: 0.049218, acc.: 100.00%] [G loss: 4.624376]\n","119 [D loss: 0.015940, acc.: 100.00%] [G loss: 4.921829]\n","120 [D loss: 0.016983, acc.: 100.00%] [G loss: 4.710429]\n","121 [D loss: 0.046611, acc.: 100.00%] [G loss: 5.517042]\n","122 [D loss: 0.090613, acc.: 98.44%] [G loss: 5.262113]\n","123 [D loss: 0.036875, acc.: 100.00%] [G loss: 5.056708]\n","124 [D loss: 0.022898, acc.: 100.00%] [G loss: 5.212194]\n","125 [D loss: 0.040129, acc.: 98.44%] [G loss: 5.127780]\n","126 [D loss: 0.031415, acc.: 98.44%] [G loss: 5.308478]\n","127 [D loss: 0.075831, acc.: 98.44%] [G loss: 4.696424]\n","128 [D loss: 0.040200, acc.: 98.44%] [G loss: 5.569962]\n","129 [D loss: 0.149318, acc.: 93.75%] [G loss: 3.759258]\n","130 [D loss: 0.117238, acc.: 96.88%] [G loss: 5.514782]\n","131 [D loss: 0.024269, acc.: 100.00%] [G loss: 5.230607]\n","132 [D loss: 0.474730, acc.: 79.69%] [G loss: 4.194086]\n","133 [D loss: 0.021750, acc.: 100.00%] [G loss: 5.125203]\n","134 [D loss: 0.053368, acc.: 100.00%] [G loss: 4.507771]\n","135 [D loss: 0.076093, acc.: 96.88%] [G loss: 4.860642]\n","136 [D loss: 0.284229, acc.: 87.50%] [G loss: 6.185729]\n","137 [D loss: 2.310633, acc.: 32.81%] [G loss: 2.801689]\n","138 [D loss: 1.039916, acc.: 70.31%] [G loss: 1.564978]\n","139 [D loss: 0.718834, acc.: 75.00%] [G loss: 1.619222]\n","140 [D loss: 0.371941, acc.: 75.00%] [G loss: 2.490924]\n","141 [D loss: 0.182126, acc.: 89.06%] [G loss: 3.121903]\n","142 [D loss: 0.107325, acc.: 93.75%] [G loss: 3.572162]\n","143 [D loss: 0.240646, acc.: 89.06%] [G loss: 3.277749]\n","144 [D loss: 0.082141, acc.: 98.44%] [G loss: 3.204857]\n","145 [D loss: 0.116601, acc.: 96.88%] [G loss: 3.145293]\n","146 [D loss: 0.058275, acc.: 100.00%] [G loss: 2.857042]\n","147 [D loss: 0.072489, acc.: 100.00%] [G loss: 2.950096]\n","148 [D loss: 0.136706, acc.: 95.31%] [G loss: 2.835256]\n","149 [D loss: 0.146537, acc.: 93.75%] [G loss: 2.898909]\n","150 [D loss: 0.121355, acc.: 95.31%] [G loss: 3.079549]\n","151 [D loss: 0.238517, acc.: 90.62%] [G loss: 2.795433]\n","152 [D loss: 0.140286, acc.: 96.88%] [G loss: 3.087633]\n","153 [D loss: 0.135480, acc.: 96.88%] [G loss: 3.155694]\n","154 [D loss: 0.170529, acc.: 96.88%] [G loss: 2.894897]\n","155 [D loss: 0.254010, acc.: 89.06%] [G loss: 2.918195]\n","156 [D loss: 0.211779, acc.: 95.31%] [G loss: 2.877556]\n","157 [D loss: 0.154853, acc.: 93.75%] [G loss: 3.004976]\n","158 [D loss: 0.150212, acc.: 95.31%] [G loss: 2.772661]\n","159 [D loss: 0.214185, acc.: 89.06%] [G loss: 2.513813]\n","160 [D loss: 0.117146, acc.: 98.44%] [G loss: 2.660480]\n","161 [D loss: 0.284097, acc.: 87.50%] [G loss: 3.058777]\n","162 [D loss: 0.277747, acc.: 92.19%] [G loss: 2.846194]\n","163 [D loss: 0.229146, acc.: 89.06%] [G loss: 3.502927]\n","164 [D loss: 0.634288, acc.: 65.62%] [G loss: 2.196121]\n","165 [D loss: 0.144654, acc.: 90.62%] [G loss: 3.464045]\n","166 [D loss: 0.116828, acc.: 98.44%] [G loss: 3.190364]\n","167 [D loss: 0.094602, acc.: 100.00%] [G loss: 3.085309]\n","168 [D loss: 0.189029, acc.: 90.62%] [G loss: 3.130978]\n","169 [D loss: 0.194455, acc.: 93.75%] [G loss: 3.218577]\n","170 [D loss: 0.265112, acc.: 87.50%] [G loss: 3.483053]\n","171 [D loss: 0.186450, acc.: 95.31%] [G loss: 2.971575]\n","172 [D loss: 0.140756, acc.: 95.31%] [G loss: 2.946786]\n","173 [D loss: 0.263227, acc.: 87.50%] [G loss: 3.045871]\n","174 [D loss: 0.233121, acc.: 90.62%] [G loss: 3.120563]\n","175 [D loss: 0.209226, acc.: 93.75%] [G loss: 3.330264]\n","176 [D loss: 0.407486, acc.: 82.81%] [G loss: 2.538519]\n","177 [D loss: 0.138700, acc.: 93.75%] [G loss: 3.494431]\n","178 [D loss: 0.314562, acc.: 87.50%] [G loss: 2.773877]\n","179 [D loss: 0.095967, acc.: 96.88%] [G loss: 3.490858]\n","180 [D loss: 0.312074, acc.: 82.81%] [G loss: 2.152054]\n","181 [D loss: 0.172115, acc.: 90.62%] [G loss: 3.751312]\n","182 [D loss: 0.181153, acc.: 100.00%] [G loss: 3.318190]\n","183 [D loss: 0.128686, acc.: 96.88%] [G loss: 3.145810]\n","184 [D loss: 0.083387, acc.: 98.44%] [G loss: 3.354542]\n","185 [D loss: 0.168798, acc.: 92.19%] [G loss: 2.983093]\n","186 [D loss: 0.142572, acc.: 95.31%] [G loss: 3.272192]\n","187 [D loss: 0.682714, acc.: 62.50%] [G loss: 2.939755]\n","188 [D loss: 0.099422, acc.: 98.44%] [G loss: 4.082469]\n","189 [D loss: 1.170564, acc.: 46.88%] [G loss: 0.970670]\n","190 [D loss: 0.727361, acc.: 60.94%] [G loss: 2.795905]\n","191 [D loss: 0.079866, acc.: 100.00%] [G loss: 4.433404]\n","192 [D loss: 0.420403, acc.: 81.25%] [G loss: 2.475164]\n","193 [D loss: 0.168828, acc.: 92.19%] [G loss: 2.809522]\n","194 [D loss: 0.130033, acc.: 96.88%] [G loss: 3.491236]\n","195 [D loss: 0.193786, acc.: 95.31%] [G loss: 3.465550]\n","196 [D loss: 0.317740, acc.: 84.38%] [G loss: 2.982786]\n","197 [D loss: 0.087800, acc.: 100.00%] [G loss: 3.667797]\n","198 [D loss: 0.332566, acc.: 82.81%] [G loss: 2.471979]\n","199 [D loss: 0.185566, acc.: 92.19%] [G loss: 3.616431]\n","200 [D loss: 0.198964, acc.: 93.75%] [G loss: 3.524037]\n","201 [D loss: 0.423490, acc.: 79.69%] [G loss: 2.989361]\n","202 [D loss: 0.211494, acc.: 90.62%] [G loss: 3.638211]\n","203 [D loss: 0.456037, acc.: 79.69%] [G loss: 3.185226]\n","204 [D loss: 0.101431, acc.: 100.00%] [G loss: 3.732814]\n","205 [D loss: 0.674054, acc.: 70.31%] [G loss: 1.797148]\n","206 [D loss: 0.351585, acc.: 79.69%] [G loss: 3.889274]\n","207 [D loss: 0.194656, acc.: 95.31%] [G loss: 3.427335]\n","208 [D loss: 0.268537, acc.: 90.62%] [G loss: 3.802613]\n","209 [D loss: 0.235183, acc.: 95.31%] [G loss: 4.081376]\n","210 [D loss: 0.573676, acc.: 70.31%] [G loss: 1.903144]\n","211 [D loss: 0.216443, acc.: 90.62%] [G loss: 3.129720]\n","212 [D loss: 0.463050, acc.: 75.00%] [G loss: 3.326448]\n","213 [D loss: 0.253791, acc.: 92.19%] [G loss: 3.490514]\n","214 [D loss: 0.592931, acc.: 73.44%] [G loss: 3.328814]\n","215 [D loss: 0.158238, acc.: 96.88%] [G loss: 3.881500]\n","216 [D loss: 0.843074, acc.: 60.94%] [G loss: 1.857012]\n","217 [D loss: 0.192665, acc.: 93.75%] [G loss: 2.950283]\n","218 [D loss: 0.675177, acc.: 60.94%] [G loss: 2.247895]\n","219 [D loss: 0.240973, acc.: 89.06%] [G loss: 3.295110]\n","220 [D loss: 0.656754, acc.: 60.94%] [G loss: 1.834548]\n","221 [D loss: 0.233451, acc.: 87.50%] [G loss: 3.612319]\n","222 [D loss: 0.798626, acc.: 59.38%] [G loss: 1.494333]\n","223 [D loss: 0.396877, acc.: 76.56%] [G loss: 3.529004]\n","224 [D loss: 0.750417, acc.: 56.25%] [G loss: 2.064884]\n","225 [D loss: 0.245552, acc.: 90.62%] [G loss: 3.444176]\n","226 [D loss: 0.508911, acc.: 78.12%] [G loss: 2.372581]\n","227 [D loss: 0.255219, acc.: 89.06%] [G loss: 2.973772]\n","228 [D loss: 0.521776, acc.: 71.88%] [G loss: 2.440461]\n","229 [D loss: 0.424776, acc.: 81.25%] [G loss: 2.442109]\n","230 [D loss: 0.319272, acc.: 87.50%] [G loss: 2.895546]\n","231 [D loss: 0.677619, acc.: 59.38%] [G loss: 1.791595]\n","232 [D loss: 0.283674, acc.: 84.38%] [G loss: 3.453794]\n","233 [D loss: 1.318336, acc.: 35.94%] [G loss: 0.710641]\n","234 [D loss: 0.525405, acc.: 62.50%] [G loss: 2.184659]\n","235 [D loss: 0.267453, acc.: 90.62%] [G loss: 3.228384]\n","236 [D loss: 1.184745, acc.: 29.69%] [G loss: 0.895202]\n","237 [D loss: 0.346671, acc.: 79.69%] [G loss: 2.454731]\n","238 [D loss: 0.642208, acc.: 57.81%] [G loss: 1.866273]\n","239 [D loss: 0.545640, acc.: 71.88%] [G loss: 1.674041]\n","240 [D loss: 0.479445, acc.: 76.56%] [G loss: 2.030771]\n","241 [D loss: 0.598215, acc.: 68.75%] [G loss: 1.954503]\n","242 [D loss: 0.747563, acc.: 46.88%] [G loss: 1.205701]\n","243 [D loss: 0.621301, acc.: 60.94%] [G loss: 1.767988]\n","244 [D loss: 0.655538, acc.: 67.19%] [G loss: 1.551130]\n","245 [D loss: 0.590516, acc.: 62.50%] [G loss: 2.020945]\n","246 [D loss: 0.722856, acc.: 56.25%] [G loss: 1.410115]\n","247 [D loss: 0.513434, acc.: 65.62%] [G loss: 2.118098]\n","248 [D loss: 0.721732, acc.: 51.56%] [G loss: 1.464456]\n","249 [D loss: 0.767776, acc.: 48.44%] [G loss: 1.749695]\n","250 [D loss: 0.623021, acc.: 64.06%] [G loss: 1.880079]\n","251 [D loss: 0.967298, acc.: 40.62%] [G loss: 1.140282]\n","252 [D loss: 0.529157, acc.: 68.75%] [G loss: 1.806532]\n","253 [D loss: 1.113380, acc.: 28.12%] [G loss: 0.741577]\n","254 [D loss: 0.575316, acc.: 59.38%] [G loss: 1.389866]\n","255 [D loss: 0.656371, acc.: 56.25%] [G loss: 1.484393]\n","256 [D loss: 0.788674, acc.: 45.31%] [G loss: 1.010467]\n","257 [D loss: 0.620519, acc.: 56.25%] [G loss: 1.315284]\n","258 [D loss: 0.747655, acc.: 48.44%] [G loss: 1.107747]\n","259 [D loss: 0.600532, acc.: 67.19%] [G loss: 1.196080]\n","260 [D loss: 0.751162, acc.: 50.00%] [G loss: 1.048003]\n","261 [D loss: 0.816787, acc.: 46.88%] [G loss: 0.992896]\n","262 [D loss: 0.763799, acc.: 46.88%] [G loss: 1.172821]\n","263 [D loss: 0.679779, acc.: 51.56%] [G loss: 1.113401]\n","264 [D loss: 0.687541, acc.: 51.56%] [G loss: 0.944852]\n","265 [D loss: 0.729368, acc.: 51.56%] [G loss: 1.051032]\n","266 [D loss: 0.623834, acc.: 59.38%] [G loss: 1.177470]\n","267 [D loss: 0.795142, acc.: 39.06%] [G loss: 0.958201]\n","268 [D loss: 0.745237, acc.: 43.75%] [G loss: 1.026984]\n","269 [D loss: 0.677237, acc.: 51.56%] [G loss: 1.077105]\n","270 [D loss: 0.821025, acc.: 40.62%] [G loss: 0.807423]\n","271 [D loss: 0.704599, acc.: 50.00%] [G loss: 0.965978]\n","272 [D loss: 0.790208, acc.: 35.94%] [G loss: 0.881299]\n","273 [D loss: 0.739879, acc.: 46.88%] [G loss: 0.766034]\n","274 [D loss: 0.709523, acc.: 45.31%] [G loss: 0.899334]\n","275 [D loss: 0.697493, acc.: 46.88%] [G loss: 0.933976]\n","276 [D loss: 0.674892, acc.: 53.12%] [G loss: 0.993007]\n","277 [D loss: 0.770994, acc.: 37.50%] [G loss: 0.856782]\n","278 [D loss: 0.716917, acc.: 50.00%] [G loss: 0.928121]\n","279 [D loss: 0.737674, acc.: 45.31%] [G loss: 0.844529]\n","280 [D loss: 0.732836, acc.: 43.75%] [G loss: 0.779916]\n","281 [D loss: 0.782008, acc.: 43.75%] [G loss: 0.733677]\n","282 [D loss: 0.736274, acc.: 45.31%] [G loss: 0.822639]\n","283 [D loss: 0.748437, acc.: 42.19%] [G loss: 0.866083]\n","284 [D loss: 0.713133, acc.: 46.88%] [G loss: 0.872434]\n","285 [D loss: 0.765977, acc.: 34.38%] [G loss: 0.803527]\n","286 [D loss: 0.711470, acc.: 45.31%] [G loss: 0.759393]\n","287 [D loss: 0.761522, acc.: 34.38%] [G loss: 0.712107]\n","288 [D loss: 0.749174, acc.: 40.62%] [G loss: 0.750940]\n","289 [D loss: 0.742978, acc.: 39.06%] [G loss: 0.732055]\n","290 [D loss: 0.720338, acc.: 43.75%] [G loss: 0.747722]\n","291 [D loss: 0.712911, acc.: 43.75%] [G loss: 0.728760]\n","292 [D loss: 0.692326, acc.: 48.44%] [G loss: 0.759341]\n","293 [D loss: 0.695558, acc.: 43.75%] [G loss: 0.803014]\n","294 [D loss: 0.719967, acc.: 43.75%] [G loss: 0.731518]\n","295 [D loss: 0.745988, acc.: 37.50%] [G loss: 0.662150]\n","296 [D loss: 0.714236, acc.: 45.31%] [G loss: 0.649401]\n","297 [D loss: 0.710099, acc.: 45.31%] [G loss: 0.664510]\n","298 [D loss: 0.700939, acc.: 43.75%] [G loss: 0.715129]\n","299 [D loss: 0.723382, acc.: 42.19%] [G loss: 0.694642]\n","300 [D loss: 0.703594, acc.: 42.19%] [G loss: 0.696773]\n","301 [D loss: 0.695720, acc.: 45.31%] [G loss: 0.675790]\n","302 [D loss: 0.713008, acc.: 45.31%] [G loss: 0.675519]\n","303 [D loss: 0.725557, acc.: 40.62%] [G loss: 0.667147]\n","304 [D loss: 0.702185, acc.: 45.31%] [G loss: 0.697122]\n","305 [D loss: 0.666361, acc.: 50.00%] [G loss: 0.694242]\n","306 [D loss: 0.690895, acc.: 45.31%] [G loss: 0.705714]\n","307 [D loss: 0.697174, acc.: 48.44%] [G loss: 0.706960]\n","308 [D loss: 0.652864, acc.: 51.56%] [G loss: 0.737141]\n","309 [D loss: 0.687548, acc.: 45.31%] [G loss: 0.738790]\n","310 [D loss: 0.724177, acc.: 35.94%] [G loss: 0.721555]\n","311 [D loss: 0.732710, acc.: 31.25%] [G loss: 0.674956]\n","312 [D loss: 0.692614, acc.: 46.88%] [G loss: 0.670939]\n","313 [D loss: 0.686236, acc.: 46.88%] [G loss: 0.680065]\n","314 [D loss: 0.665844, acc.: 50.00%] [G loss: 0.708155]\n","315 [D loss: 0.669982, acc.: 48.44%] [G loss: 0.716171]\n","316 [D loss: 0.678437, acc.: 48.44%] [G loss: 0.715894]\n","317 [D loss: 0.681622, acc.: 45.31%] [G loss: 0.696229]\n","318 [D loss: 0.682464, acc.: 48.44%] [G loss: 0.710607]\n","319 [D loss: 0.683796, acc.: 48.44%] [G loss: 0.703959]\n","320 [D loss: 0.668382, acc.: 51.56%] [G loss: 0.695472]\n","321 [D loss: 0.705418, acc.: 46.88%] [G loss: 0.680258]\n","322 [D loss: 0.650252, acc.: 50.00%] [G loss: 0.703486]\n","323 [D loss: 0.729146, acc.: 37.50%] [G loss: 0.672418]\n","324 [D loss: 0.678589, acc.: 46.88%] [G loss: 0.683977]\n","325 [D loss: 0.687523, acc.: 50.00%] [G loss: 0.685595]\n","326 [D loss: 0.677040, acc.: 48.44%] [G loss: 0.681607]\n","327 [D loss: 0.673627, acc.: 46.88%] [G loss: 0.671702]\n","328 [D loss: 0.711656, acc.: 40.62%] [G loss: 0.660786]\n","329 [D loss: 0.655252, acc.: 46.88%] [G loss: 0.671697]\n","330 [D loss: 0.663480, acc.: 50.00%] [G loss: 0.722378]\n","331 [D loss: 0.662566, acc.: 48.44%] [G loss: 0.711859]\n","332 [D loss: 0.662749, acc.: 51.56%] [G loss: 0.702574]\n","333 [D loss: 0.691362, acc.: 43.75%] [G loss: 0.672560]\n","334 [D loss: 0.671782, acc.: 51.56%] [G loss: 0.694204]\n","335 [D loss: 0.676824, acc.: 45.31%] [G loss: 0.684541]\n","336 [D loss: 0.662768, acc.: 46.88%] [G loss: 0.682197]\n","337 [D loss: 0.677574, acc.: 48.44%] [G loss: 0.717549]\n","338 [D loss: 0.675293, acc.: 50.00%] [G loss: 0.708469]\n","339 [D loss: 0.662802, acc.: 53.12%] [G loss: 0.690701]\n","340 [D loss: 0.678790, acc.: 46.88%] [G loss: 0.687246]\n","341 [D loss: 0.659926, acc.: 46.88%] [G loss: 0.694812]\n","342 [D loss: 0.686926, acc.: 45.31%] [G loss: 0.691694]\n","343 [D loss: 0.669166, acc.: 46.88%] [G loss: 0.671275]\n","344 [D loss: 0.679165, acc.: 46.88%] [G loss: 0.690427]\n","345 [D loss: 0.657828, acc.: 45.31%] [G loss: 0.677016]\n","346 [D loss: 0.666388, acc.: 50.00%] [G loss: 0.673386]\n","347 [D loss: 0.650488, acc.: 48.44%] [G loss: 0.684390]\n","348 [D loss: 0.677270, acc.: 43.75%] [G loss: 0.671903]\n","349 [D loss: 0.647367, acc.: 50.00%] [G loss: 0.674987]\n","350 [D loss: 0.678552, acc.: 46.88%] [G loss: 0.694292]\n","351 [D loss: 0.656369, acc.: 48.44%] [G loss: 0.699888]\n","352 [D loss: 0.648050, acc.: 54.69%] [G loss: 0.705435]\n","353 [D loss: 0.658112, acc.: 45.31%] [G loss: 0.697747]\n","354 [D loss: 0.657500, acc.: 48.44%] [G loss: 0.708396]\n","355 [D loss: 0.654484, acc.: 51.56%] [G loss: 0.700709]\n","356 [D loss: 0.650769, acc.: 53.12%] [G loss: 0.714284]\n","357 [D loss: 0.649931, acc.: 48.44%] [G loss: 0.687110]\n","358 [D loss: 0.662686, acc.: 46.88%] [G loss: 0.689956]\n","359 [D loss: 0.669474, acc.: 46.88%] [G loss: 0.676396]\n","360 [D loss: 0.682139, acc.: 42.19%] [G loss: 0.669567]\n","361 [D loss: 0.614947, acc.: 51.56%] [G loss: 0.693476]\n","362 [D loss: 0.656444, acc.: 50.00%] [G loss: 0.697923]\n","363 [D loss: 0.661036, acc.: 48.44%] [G loss: 0.704041]\n","364 [D loss: 0.651027, acc.: 50.00%] [G loss: 0.693972]\n","365 [D loss: 0.671039, acc.: 50.00%] [G loss: 0.679456]\n","366 [D loss: 0.628519, acc.: 53.12%] [G loss: 0.687473]\n","367 [D loss: 0.663410, acc.: 42.19%] [G loss: 0.693076]\n","368 [D loss: 0.681315, acc.: 51.56%] [G loss: 0.682416]\n","369 [D loss: 0.668560, acc.: 51.56%] [G loss: 0.689238]\n","370 [D loss: 0.623651, acc.: 53.12%] [G loss: 0.713750]\n","371 [D loss: 0.631672, acc.: 57.81%] [G loss: 0.706958]\n","372 [D loss: 0.657232, acc.: 48.44%] [G loss: 0.706256]\n","373 [D loss: 0.677216, acc.: 50.00%] [G loss: 0.702233]\n","374 [D loss: 0.684184, acc.: 46.88%] [G loss: 0.697965]\n","375 [D loss: 0.642886, acc.: 48.44%] [G loss: 0.691453]\n","376 [D loss: 0.675742, acc.: 51.56%] [G loss: 0.673317]\n","377 [D loss: 0.665276, acc.: 51.56%] [G loss: 0.685819]\n","378 [D loss: 0.651285, acc.: 51.56%] [G loss: 0.687913]\n","379 [D loss: 0.650725, acc.: 48.44%] [G loss: 0.708422]\n","380 [D loss: 0.653905, acc.: 56.25%] [G loss: 0.680292]\n","381 [D loss: 0.673932, acc.: 50.00%] [G loss: 0.717799]\n","382 [D loss: 0.697125, acc.: 45.31%] [G loss: 0.701963]\n","383 [D loss: 0.657795, acc.: 57.81%] [G loss: 0.740014]\n","384 [D loss: 0.670286, acc.: 51.56%] [G loss: 0.712858]\n","385 [D loss: 0.641323, acc.: 60.94%] [G loss: 0.716886]\n","386 [D loss: 0.685741, acc.: 48.44%] [G loss: 0.718278]\n","387 [D loss: 0.699589, acc.: 45.31%] [G loss: 0.716173]\n","388 [D loss: 0.691706, acc.: 46.88%] [G loss: 0.695564]\n","389 [D loss: 0.676352, acc.: 51.56%] [G loss: 0.700602]\n","390 [D loss: 0.647416, acc.: 56.25%] [G loss: 0.698388]\n","391 [D loss: 0.643259, acc.: 62.50%] [G loss: 0.697922]\n","392 [D loss: 0.670212, acc.: 48.44%] [G loss: 0.685850]\n","393 [D loss: 0.666234, acc.: 50.00%] [G loss: 0.686346]\n","394 [D loss: 0.649176, acc.: 53.12%] [G loss: 0.695400]\n","395 [D loss: 0.678037, acc.: 50.00%] [G loss: 0.690577]\n","396 [D loss: 0.684506, acc.: 45.31%] [G loss: 0.692160]\n","397 [D loss: 0.689393, acc.: 43.75%] [G loss: 0.711806]\n","398 [D loss: 0.654070, acc.: 50.00%] [G loss: 0.684139]\n","399 [D loss: 0.688789, acc.: 51.56%] [G loss: 0.689809]\n","400 [D loss: 0.661885, acc.: 48.44%] [G loss: 0.702943]\n","--- 21.472412586212158 seconds ---\n"],"name":"stdout"}]},{"metadata":{"id":"_jdmVRjoyC1z","colab_type":"code","colab":{}},"cell_type":"code","source":["# Sample live plot\n","import time\n","import pylab as pl\n","from IPython import display\n","for i in range(3):\n","    pl.cla()\n","    pl.plot(pl.randn(100))\n","    display.clear_output(wait=False)\n","    display.display(pl.gcf())\n","    time.sleep(1.0)\n","display.clear_output(wait=True)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Jd18wCLROPlO","colab_type":"code","colab":{}},"cell_type":"code","source":["# https://colab.research.google.com/github/irhallac/deep_learning_examples/blob/master/CIFAR_TENSORBOARD_COLAB.ipynb#scrollTo=1Uf3VZ91xzpw\n","\n","# set paths\n","ROOT = %pwd\n","LOG_DIR = os.path.join(ROOT, 'log1')\n","\n","# will install `ngrok`, if necessary\n","# will create `log_dir` if path does not exist\n","launch_tensorboard( bin_dir=ROOT, log_dir=LOG_DIR )\n","\n","tensorboard = TensorBoard(log_dir=LOG_DIR, histogram_freq=0, write_graph=True, write_images=True)\n","# history = model.fit(X_train, Y_train, batch_size=BATCH_SIZE, epochs=NB_EPOCH, validation_split=VALIDATION_SPLIT, callbacks=[tensorboard], verbose=VERBOSE)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"FNi3VShROVcQ","colab_type":"code","outputId":"6b7f38da-e36c-404b-892c-59462290b57c","executionInfo":{"status":"ok","timestamp":1545521896392,"user_tz":300,"elapsed":4088,"user":{"displayName":"Ali Panahi","photoUrl":"https://lh5.googleusercontent.com/-aAEVDWwWDxU/AAAAAAAAAAI/AAAAAAAAEuM/WOpI1BZp8tM/s64/photo.jpg","userId":"09333706783536538061"}},"colab":{"base_uri":"https://localhost:8080/","height":136}},"cell_type":"code","source":["!ssh"],"execution_count":0,"outputs":[{"output_type":"stream","text":["usage: ssh [-46AaCfGgKkMNnqsTtVvXxYy] [-b bind_address] [-c cipher_spec]\n","           [-D [bind_address:]port] [-E log_file] [-e escape_char]\n","           [-F configfile] [-I pkcs11] [-i identity_file]\n","           [-J [user@]host[:port]] [-L address] [-l login_name] [-m mac_spec]\n","           [-O ctl_cmd] [-o option] [-p port] [-Q query_option] [-R address]\n","           [-S ctl_path] [-W host:port] [-w local_tun[:remote_tun]]\n","           [user@]hostname [command]\n"],"name":"stdout"}]},{"metadata":{"id":"2ZSFZ-6lSHla","colab_type":"code","outputId":"e0cc9b0c-1b59-4965-fd41-e720930d7df4","executionInfo":{"status":"ok","timestamp":1545522074490,"user_tz":300,"elapsed":23290,"user":{"displayName":"Ali Panahi","photoUrl":"https://lh5.googleusercontent.com/-aAEVDWwWDxU/AAAAAAAAAAI/AAAAAAAAEuM/WOpI1BZp8tM/s64/photo.jpg","userId":"09333706783536538061"}},"colab":{"base_uri":"https://localhost:8080/","height":119}},"cell_type":"code","source":["!ssh -o StrictHostKeyChecking=no -R 80:localhost:3000 serveo.net"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Warning: Permanently added 'serveo.net,159.89.214.31' (RSA) to the list of known hosts.\r\n","Hi there\r\n","\u001b[32mForwarding HTTP traffic from https://adaugeo.serveo.net\r\n","\u001b[0mPress g to start a GUI session and ctrl-c to quit.\n","HTTP request from 73.99.95.71 to https://adaugeo.serveo.net/\n","connect_to localhost port 3000: failed.\n"],"name":"stdout"}]},{"metadata":{"id":"57yuL71DSNz-","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}