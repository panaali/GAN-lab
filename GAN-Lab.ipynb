{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"GAN-Lab.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":["zThX0FEaU7-H","Jj3xK9uvUebh","C2nXLNdf3WhC","VJ98gSX5V0lx","wmKFD_EZ3FJZ","2nA7Zx-sS18O","i1r_IUhttMZp","c4u2Exm6tQb4","wiVzINXGVwFr"],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"5s4EcwwZbnwc","colab_type":"text"},"cell_type":"markdown","source":["# GAN LAB\n","By: Ali Panahi\n","\n","Inspired by [GAN](https://github.com/eriklindernoren/Keras-GAN/blob/master/gan/gan.py),\n","[DCGAN](https://github.com/eriklindernoren/Keras-GAN/blob/master/dcgan/dcgan.py), [CycleGAN](https://github.com/eriklindernoren/Keras-GAN/blob/master/cyclegan/cyclegan.py)\n","[DCGAN by fchaollet](https://colab.research.google.com/github/fchollet/deep-learning-with-python-notebooks/blob/master/8.5-introduction-to-gans.ipynb#scrollTo=WLCF7I8opN7i), [DCGAN from TensorFlow with eager](https://github.com/tensorflow/tensorflow/blob/r1.11/tensorflow/contrib/eager/python/examples/generative_examples/dcgan.ipynb)\n"]},{"metadata":{"id":"hOtc-V6p4D6x","colab_type":"text"},"cell_type":"markdown","source":["# Document"]},{"metadata":{"id":"zThX0FEaU7-H","colab_type":"text"},"cell_type":"markdown","source":["## How to use this Colab notebook\n"]},{"metadata":{"colab_type":"text","id":"sm95tNZEUYDy"},"cell_type":"markdown","source":["- Go to File > Save a copy in Drive\n","- Runtime > Change Runtime type > GPU (Free Tesla K80 GPU)\n","- Go to Prepare > User Defined Variables and set `GOOGLE_DRIVE_ENABLED = FALSE` if you don't need a persistant storage\n","- Run All Cell\n","- Go to TensorBoard > Click on the TensorBoard link to see how well your model is working\n","- Change Parameters or architechture and Run again, You can compare the performance in TensorBoard\n","- For **debugging** the code you can add `import pdb; pdb.set_trace()`  to any line of the code as a *breakpoint* and then run the code. below you can see a list of some of the commands that you can use while you're debugging your code. [Tutorial article on PDB](https://www.machinelearningplus.com/python/python-debugging/) , [Tutorial Video on PDB](https://www.youtube.com/watch?v=VQjCx3P89yk)\n","- For **debugging** with GUI you can add `import web_pdb; web_pdb.set_trace()` to any line of the code as a *breakpoint* and then run the code. Then visit the link provided bellow in Debugging section . \n"]},{"metadata":{"id":"dRnRHJgxLIOL","colab_type":"text"},"cell_type":"markdown","source":["\n","\n","### PDB list of debugging commands\n","| Command         |  Description                                                |\n","|-----------------|-------------------------------------------------------------|\n","| ``list``        | Show the current location in the file                       |\n","| ``h(elp)``      | Show a list of commands, or find help on a specific command |\n","| ``q(uit)``      | Quit the debugger and the program                           |\n","| ``c(ontinue)``  | Quit the debugger, continue in the program                  |\n","| ``n(ext)``      | Go to the next step of the program                          |\n","| ``<enter>``     | Repeat the previous command                                 |\n","| ``p(rint)``     | Print variables                                             |\n","| ``s(tep)``      | Step into a subroutine                                      |\n","| ``r(eturn)``    | Return out of a subroutine                                  |\n"]},{"metadata":{"colab_type":"text","id":"Jj3xK9uvUebh"},"cell_type":"markdown","source":["## How the original GAN works\n"]},{"metadata":{"colab_type":"text","id":"EbLR4akoUebc"},"cell_type":"markdown","source":["![Gan Architecture](https://ishmaelbelghazi.github.io/ALI/assets/gan_simple.svg)\n","Image Source [ALI](https://ishmaelbelghazi.github.io/ALI)\n","\n","- for each epoch:\n","  1.   Generator creates fake images (generator.predict)\n","  2.   Discriminator learns how to distinguish these fake images with real images (discriminator.train_on_batch)\n","  3.   Generator learns how to create better fake images using Discriminator guides ${\\frac{p_{data}(x)}{  p_{model}(x)}}$ (combo.train_on_batch)\n","\n","So generator generates images that discrimintator cannot distinguish => combo low score!\n","\n","\n","### Theoretical perfect final state\n","-   Have 100% accuracy for the generator - meaning the discriminator classifies all synthetic observations as real;\n","-   Have about 50% accuracy for the discriminator - meaning it cannot distingiush fake observations from real ones;\n","-   The synthetic observations are of good quality."]},{"metadata":{"id":"mDELg4b8trv5","colab_type":"text"},"cell_type":"markdown","source":["## How CycleGAN Works"]},{"metadata":{"id":"gTO0Y4Z1tzx6","colab_type":"text"},"cell_type":"markdown","source":["### CycleGAN Architecture \n","![CycleGan Architecture](https://camo.githubusercontent.com/c653ddc55471557b851a7059540e80799fad7e29/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f6379636c6567616e2e706e67)\n","Image Source: [KerasGAN](https://github.com/eriklindernoren/Keras-GAN)\n","\n","![CycleGAN Ali](https://i.imgur.com/5eMUAo2.png)\n","Image By Ali Panahi\n","### CycleGAN Generator/Discriminitator Networks\n","What you should know about the generator and the discriminator networks:\n","**Discriminator**\n","-  [PatchGAN](https://arxiv.org/pdf/1611.07004.pdf)\n","  - Only penalizes structure at the scale of image patches\n","  - In order to model high-frequencies, it is sufficient to restrict our attention to the structure in local image patches.\n","  - PatchGAN's Discriminator tries to classify if each N Ã— N patch in an image is real or fake. We run this discriminator convolutationally across the image, averaging all responses to provide the ultimate output of D. (like  texture/style loss)\n","  - Using the Patch GAN Approach we can train and generate high resolution images\n","\n","**Generator**\n","- Original CycleGAN generator: [Perceptual Losses](Perceptual Losses for Real-Time Style Transfer\n","and Super-Resolution)\n","- New CycleGAN generator: [U-net]()\n","  - Using U-net as a generator has been a big improvement for forwarding low level features through the network and partially reconstructing it at the output.\n","- [Layer Normalization](https://arxiv.org/abs/1607.06450) /  [Instance Normalizatin Layer](https://arxiv.org/abs/1607.08022)\n","  - Normalize the activations of the previous layer at each step, i.e. applies a transformation that maintains the mean activation close to 0 and the activation standard deviation close to 1.\n","- [CycleGAN Identity Loss](https://arxiv.org/pdf/1611.02200.pdf)\n","  - Regularize the generator to be near an identity mapping when real samples of the target domain are provided as the input to the gen- erator\n","\n","![U-net](http://deeplearning.net/tutorial/_images/unet.jpg)\n","\n","Image source: [U-net](http://deeplearning.net/tutorial/_images/unet.jpg)\n","\n","**Lambda**\n","\n","We used these lambdas for the importance of each of the losses in the final loss function:\n","\n","```g_loss = 1 * adversarial losses + 10 * Cycle-consistency losses + 1 * Identity losses```\n","\n","**Model names in Keras output**\n","\n","In tensorboard you will see loss and accuracies named by models name of ```model_1, mode_2, model_3, model_4```. These are belonging to: ```Adversarial_D_A, Adversarial_D_B, recon_BA, recon_AB, id_BA, id_AB```\n","\n","## Different Normalization Methods\n","\n","  -  [Batch normalization layer](https://arxiv.org/pdf/1502.03167.pdf)\n","  -  [Layer Normalization](https://arxiv.org/abs/1607.06450)\n","  -  [Instance Normalizatin Layer](https://arxiv.org/abs/1607.08022)\n","  - [Pixel Normalization](https://arxiv.org/pdf/1710.10196.pdf) (Nvidia's ProgresssiveGAN - Style Based GAN)"]},{"metadata":{"id":"IfPyWsDINUvT","colab_type":"text"},"cell_type":"markdown","source":["# Code"]},{"metadata":{"id":"17aRibZ7AQto","colab_type":"text"},"cell_type":"markdown","source":["## Preapare"]},{"metadata":{"id":"C2nXLNdf3WhC","colab_type":"text"},"cell_type":"markdown","source":["### Installing Dependencies"]},{"metadata":{"colab_type":"code","id":"RZn1TNpMwTa6","colab":{}},"cell_type":"code","source":["def install_default_packages():\n","  try:\n","     import keras_contrib\n","  except:\n","    !pip install git+https://www.github.com/keras-team/keras-contrib.git -q\n","    \n","  !pip install imageio -q\n","  !pip install scikit-image -q\n","\n","def clean_content_folder():\n","  # Clean the folder\n","  import os\n","  if os.path.exists('/content/sample_data'):\n","    !rm -rf 'sample_data'\n","\n","try: dependencies_cell_executed\n","except NameError:\n","  install_default_packages()\n","  clean_content_folder()\n","dependencies_cell_executed = True"],"execution_count":0,"outputs":[]},{"metadata":{"id":"gP5RwTJhNVD-","colab_type":"text"},"cell_type":"markdown","source":["### Import"]},{"metadata":{"id":"ucK1ox0fNXML","colab_type":"code","outputId":"8fcbae72-8449-442c-a636-fc86571610c8","executionInfo":{"status":"ok","timestamp":1548736110943,"user_tz":300,"elapsed":23601,"user":{"displayName":"Ali Panahi","photoUrl":"https://lh5.googleusercontent.com/-aAEVDWwWDxU/AAAAAAAAAAI/AAAAAAAAEuM/WOpI1BZp8tM/s64/photo.jpg","userId":"09333706783536538061"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.python import debug as tf_debug\n","import keras\n","\n","from keras_contrib.layers.normalization import InstanceNormalization\n","from keras.datasets import mnist\n","from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Concatenate\n","from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n","from keras.layers import LeakyReLU\n","from keras.layers import UpSampling2D, Conv2D\n","from keras.models import Sequential, Model\n","from keras.optimizers import Adam\n","from keras.callbacks import TensorBoard\n","import matplotlib.pyplot as plt\n","\n","import sys\n","import os\n","import time\n","import datetime\n","import numpy as np\n","from distutils.dir_util import copy_tree"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"metadata":{"id":"VJ98gSX5V0lx","colab_type":"text"},"cell_type":"markdown","source":["### Persistant Storage - Google Drive"]},{"metadata":{"id":"hLGPOYE4QVuk","colab_type":"code","colab":{}},"cell_type":"code","source":["# Add Google Drive as a persistant storage\n","def mount_Google_Drive(GDRIVE_PROJECT_PATH):\n","  from google.colab import drive\n","  import tensorflow as tf\n","  if not tf.gfile.Exists('/content/GDrive/'):\n","    tf.gfile.MakeDirs('/content/GDrive/')\n","    drive.mount('/content/GDrive/')\n","  return '/content/GDrive/My Drive/' + GDRIVE_PROJECT_PATH"],"execution_count":0,"outputs":[]},{"metadata":{"id":"wmKFD_EZ3FJZ","colab_type":"text"},"cell_type":"markdown","source":["### TensorBoard"]},{"metadata":{"id":"2X9D72ONYlde","colab_type":"code","colab":{}},"cell_type":"code","source":["# Install Dependencies\n","def tensorboard_installer():\n","  tensorboardcolab_version = !pip show tensorboardcolab | grep Version\n","  if tensorboardcolab_version != ['Version: 0.0.21.1b0']:\n","    !pip install git+https://github.com/panaali/tensorboardcolab -q\n","\n","# Run Tensorboard\n","def tensorboard_runner(Tensorboard_Path, Enable_Tensorboard_Debugger):\n","  from tensorboardcolab import TensorBoardColab\n","  if Enable_Tensorboard_Debugger is True:\n","    tbc = TensorBoardColab(graph_path = Tensorboard_Path, debugger_port=6064)\n","  else:\n","    tbc = TensorBoardColab(graph_path = Tensorboard_Path)\n","  return tbc"],"execution_count":0,"outputs":[]},{"metadata":{"id":"2nA7Zx-sS18O","colab_type":"text"},"cell_type":"markdown","source":["### Debugging with Web-PDB and PDB"]},{"metadata":{"id":"LZ9XKzIcPaM3","colab_type":"text"},"cell_type":"markdown","source":["The pdb \n","To install and run [web-pdb](https://github.com/romanvm/python-web-pdb) set the option in the configuration section and click on the \n","\n","- To add a *breakpoint*  usin web-pdb add `import web_pdb; web_pdb.set_trace()` in any line of the code you like.\n","\n","- To add a *breakpoint*  usin pdb add `import pdb; pdb.set_trace()` in any line of the code you like.\n"]},{"metadata":{"id":"upElVfIJS3-L","colab_type":"code","colab":{}},"cell_type":"code","source":["def web_pdb_installer():\n","  # setup web-pdb\n","  !pip install web-pdb\n","  !apt-get install autossh"],"execution_count":0,"outputs":[]},{"metadata":{"id":"lt3jLFrfTQi8","colab_type":"code","colab":{}},"cell_type":"code","source":["def web_pdb_runner(web_pdb_connected = False):\n","  import time\n","  if not web_pdb_connected:\n","    web_pdb_connected = True\n","    !nohup autossh -M 0 -o StrictHostKeyChecking=no -R 80:localhost:5555 serveo.net o > web_pdb.txt 2>&1 < /dev/null &\n","    time.sleep(3)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"C_oFjjxH4LMe","colab_type":"text"},"cell_type":"markdown","source":["## Architectures\n","  \n"]},{"metadata":{"id":"UfgwiCq7yy3i","colab_type":"text"},"cell_type":"markdown","source":["### AbstractGAN"]},{"metadata":{"id":"lU8_YArQy4Up","colab_type":"text"},"cell_type":"markdown","source":["Here you can find an abstract class for GANs which holds some commond functions:"]},{"metadata":{"id":"8nrUD3XAy01C","colab_type":"code","colab":{}},"cell_type":"code","source":["class AbstractGAN:\n","  def __init__(self, run_image_path, run_tensorboard_path, dataset_name, TensorBoardDebugWrapperSession = None, GDrive_run_image_path = '', GDrive_run_tensorboard_path = ''):\n","    self.run_image_path = run_image_path\n","    self.run_tensorboard_path = run_tensorboard_path\n","    self.dataset_name = dataset_name\n","    self.GDrive_run_image_path = GDrive_run_image_path\n","    self.GDrive_run_tensorboard_path = GDrive_run_tensorboard_path\n","    if TensorBoardDebugWrapperSession is not None:\n","      keras.backend.set_session(tf_debug.TensorBoardDebugWrapperSession(tf.Session(), TensorBoardDebugWrapperSession, send_traceback_and_source_code=False))\n","    self.tensorboard_folder_run = run_tensorboard_path\n","    np.random.seed(seed=0)\n","    self.start()\n","    \n","  def start(self):\n","    pass\n","  \n","  # Transform train_on_batch return value\n","  # to dict expected by on_batch_end or on_epoch_end callback\n","  def named_logs(self, model, logs):\n","    result = {}\n","    for metric_name, log_value in zip(model.metrics_names, logs):\n","      result[metric_name] = log_value\n","    result['size'] = 1\n","    return result\n","  \n","  def init_tensorboard(self, model, model_name):\n","    tensorboard = TensorBoard(\n","      log_dir= f'{self.run_tensorboard_path}/{model_name}',\n","      histogram_freq=0,\n","      batch_size=self.batch_size,\n","      write_graph=True,\n","      write_grads=True,\n","      update_freq=1\n","    )\n","    tensorboard.set_model(model)\n","    return tensorboard\n","  \n","  def copy_to_gdrive(self):\n","    if self.GDrive_run_image_path is not '':\n","      copy_tree(self.run_image_path, self.GDrive_run_image_path)\n","      copy_tree(self.run_tensorboard_path, self.GDrive_run_tensorboard_path)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"i1r_IUhttMZp","colab_type":"text"},"cell_type":"markdown","source":["### OriginalGAN"]},{"metadata":{"id":"kkKsmDSucJu6","colab_type":"code","colab":{}},"cell_type":"code","source":["class OriginalGAN(AbstractGAN):\n","  def start(self):\n","    self.img_rows = 28\n","    self.img_cols = 28\n","    self.channels = 1\n","    self.img_shape = (self.img_rows, self.img_cols, self.channels)\n","    self.latent_dim = 100\n","\n","    g_optimizer = Adam(lr=0.001, beta_1=0.5, beta_2=0.999, epsilon=None, decay=0.001, amsgrad=False)\n","    d_optimizer = Adam(lr=0.001, beta_1=0.5, beta_2=0.999, epsilon=None, decay=0.001, amsgrad=False)\n","\n","    # Build and compile the discriminator\n","    self.discriminator = self.build_discriminator()    \n","    self.discriminator.compile(loss='binary_crossentropy',\n","      optimizer=d_optimizer,\n","      metrics=['accuracy'])\n","    \n","    # Build the generator\n","    self.generator = self.build_generator()\n","\n","    # For the combined model we will only train the generator\n","    discriminator_frozen = Model(inputs=self.discriminator.inputs,\n","                                         outputs=self.discriminator.outputs)\n","    discriminator_frozen.trainable = False\n","    \n","    # The combined model  (stacked generator and discriminator)\n","    # Trains the generator to fool the discriminator\n","    combined_output = discriminator_frozen(self.generator.outputs) \n","    self.combined = Model(inputs=self.generator.inputs, outputs=combined_output)\n","      \n","    self.combined.compile(loss='binary_crossentropy',\n","      optimizer=g_optimizer,\n","      metrics=['accuracy'])\n","    \n","\n","  def build_generator(self):\n","    g_input_noise = Input(shape=(self.latent_dim,), name='g_input_noise')\n","    x = Dense(256, input_dim=self.latent_dim)(g_input_noise)\n","    x = LeakyReLU(alpha=0.2)(x)\n","    x = BatchNormalization(momentum=0.8)(x)\n","    x = Dense(512)(x)\n","    x = LeakyReLU(alpha=0.2)(x)\n","    x = BatchNormalization(momentum=0.8)(x)\n","    x = Dense(1024)(x)\n","    x = LeakyReLU(alpha=0.2)(x)\n","    x = BatchNormalization(momentum=0.8)(x)\n","    x = Dense(np.prod(self.img_shape), activation='tanh')(x)\n","    g_output_img = Reshape(self.img_shape)(x)\n","    \n","    return Model(inputs=g_input_noise, outputs=g_output_img)\n","\n","  def build_discriminator(self):\n","    d_input_img = Input(shape=self.img_shape, name='d_input_img')\n","    x = Flatten(input_shape=self.img_shape)(d_input_img)\n","    x = Dense(512)(x)\n","    x = LeakyReLU(alpha=0.2)(x)\n","    x = Dense(256)(x)\n","    x = LeakyReLU(alpha=0.2)(x)\n","    d_output_validity = Dense(1, activation='sigmoid')(x)\n","    \n","    return Model(inputs=d_input_img, outputs=d_output_validity)\n","\n","  def train(self, epochs, batch_size=128, sample_interval=50):\n","    if False:\n","      print('Generator Model:')\n","      self.discriminator.summary()\n","      print('Discriminator Model:')\n","      self.combined.summary()\n","\n","    # Load the dataset\n","    (X_train, _), (_, _) = mnist.load_data()\n","    \n","    # Rescale -1 to 1\n","    X_train = X_train / 127.5 - 1.\n","    X_train = np.expand_dims(X_train, axis=3)\n","\n","    # Adversarial ground truths\n","    valid = np.ones((batch_size, 1))\n","    fake = np.zeros((batch_size, 1))\n","\n","    # Setup tensorboard\n","    # Create the TensorBoard callback,\n","    # which we will drive manually\n","    now = time.time()\n","    tensorboard_d = TensorBoard(\n","      log_dir= f'{self.run_tensorboard_path}/{now}/d',\n","      histogram_freq=0,\n","      batch_size=batch_size,\n","      write_graph=True,\n","      write_grads=True\n","    )\n","    \n","    tensorboard_g = TensorBoard(\n","      log_dir= f'{self.run_tensorboard_path}/{now}/g',\n","      histogram_freq=0,\n","      batch_size=batch_size,\n","      write_graph=True,\n","      write_grads=True\n","    )\n","    \n","    tensorboard_d.set_model(self.discriminator)\n","    tensorboard_g.set_model(self.combined)\n","    \n","    for epoch in range(epochs):\n","\n","      # ---------------------\n","      #  Train Discriminator\n","      # ---------------------\n","      \n","      # Generate a batch of new images\n","      noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n","      gen_imgs = self.generator.predict(noise)\n","      \n","      # Select a random batch of images\n","      idx = np.random.randint(0, X_train.shape[0], batch_size)\n","      imgs = X_train[idx]\n","      \n","\n","      # Train the discriminator\n","      d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n","      d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n","      d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n","      \n","      # ---------------------\n","      #  Train Generator\n","      # ---------------------\n","      \n","      noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n","\n","      # Train the generator (to have the discriminator label samples as valid)\n","      g_loss = self.combined.train_on_batch(noise, valid)\n","      \n","      # Save the progress\n","      # print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f, acc.: %.2f%]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss[0], g_loss[1]))\n","      tensorboard_d.on_epoch_end(epoch, self.named_logs(self.discriminator, d_loss))\n","      tensorboard_g.on_epoch_end(epoch, self.named_logs(self.combined, g_loss))\n","      \n","      # If at save interval => save generated image samples\n","      if epoch % sample_interval == 0:\n","        print(f'epoch: {epoch}')\n","        self.sample_images(epoch)\n","      \n","    tensorboard_d.on_train_end(None)\n","    tensorboard_g.on_train_end(None)\n","\n","\n","  def sample_images(self, epoch):\n","    r, c = 5, 5\n","    noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n","    gen_imgs = self.generator.predict(noise)\n","\n","    # Rescale images 0 - 1\n","    gen_imgs = 0.5 * gen_imgs + 0.5\n","\n","    fig, axs = plt.subplots(r, c)\n","    cnt = 0\n","    for i in range(r):\n","      for j in range(c):\n","        axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n","        axs[i,j].axis('off')\n","        cnt += 1\n","    fig.savefig(self.image_path + f'{epoch}.png')\n","    plt.close()\n","  \n","  # Transform train_on_batch return value\n","  # to dict expected by on_batch_end or on_epoch_end callback\n","  def named_logs(self, model, logs):\n","    result = {}\n","    for metric_name, log_value in zip(model.metrics_names, logs):\n","      result[metric_name] = log_value\n","    return result"],"execution_count":0,"outputs":[]},{"metadata":{"id":"c4u2Exm6tQb4","colab_type":"text"},"cell_type":"markdown","source":["### DCGAN"]},{"metadata":{"id":"If1XzhWjtUgO","colab_type":"code","colab":{}},"cell_type":"code","source":["class DCGAN(AbstractGAN):\n","  def start(self):\n","    self.img_rows = 28\n","    self.img_cols = 28\n","    self.channels = 1\n","    self.img_shape = (self.img_rows, self.img_cols, self.channels)\n","    self.latent_dim = 100\n","\n","    g_optimizer = Adam(lr=0.0002, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0, amsgrad=True)\n","    d_optimizer = Adam(lr=0.0002, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0, amsgrad=False)\n","\n","    # Build and compile the discriminator\n","    self.discriminator = self.build_discriminator()    \n","    self.discriminator.compile(loss='binary_crossentropy',\n","      optimizer=d_optimizer,\n","      metrics=['accuracy'])\n","    \n","    # Build the generator\n","    self.generator = self.build_generator()\n","\n","    # For the combined model we will only train the generator\n","    discriminator_frozen = Model(inputs=self.discriminator.inputs,\n","                                         outputs=self.discriminator.outputs)\n","    discriminator_frozen.trainable = False\n","    \n","    # The combined model  (stacked generator and discriminator)\n","    # Trains the generator to fool the discriminator\n","    combined_output = discriminator_frozen(self.generator.outputs) \n","    self.combined = Model(inputs=self.generator.inputs, outputs=combined_output)\n","      \n","    self.combined.compile(loss='binary_crossentropy',\n","      optimizer=g_optimizer,\n","      metrics=['accuracy'])\n","    \n","\n","  def build_generator(self):\n","    model = Sequential()\n","\n","    model.add(Dense(128 * 7 * 7, activation=\"relu\", input_dim=self.latent_dim))\n","    model.add(Reshape((7, 7, 128)))\n","    model.add(UpSampling2D())\n","    model.add(Conv2D(128, kernel_size=3, padding=\"same\"))\n","    model.add(BatchNormalization(momentum=0.8))\n","    model.add(Activation(\"relu\"))\n","    model.add(UpSampling2D())\n","    model.add(Conv2D(64, kernel_size=3, padding=\"same\"))\n","    model.add(BatchNormalization(momentum=0.8))\n","    model.add(Activation(\"relu\"))\n","    model.add(Conv2D(self.channels, kernel_size=3, padding=\"same\"))\n","    model.add(Activation(\"tanh\"))\n","\n","    noise = Input(shape=(self.latent_dim,))\n","    img = model(noise)\n","\n","    return Model(noise, img)\n","\n","  def build_discriminator(self):\n","    model = Sequential()\n","\n","    model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=self.img_shape, padding=\"same\"))\n","    model.add(LeakyReLU(alpha=0.2))\n","    model.add(Dropout(0.25))\n","    model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n","    model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n","    model.add(BatchNormalization(momentum=0.8))\n","    model.add(LeakyReLU(alpha=0.2))\n","    model.add(Dropout(0.25))\n","    model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n","    model.add(BatchNormalization(momentum=0.8))\n","    model.add(LeakyReLU(alpha=0.2))\n","    model.add(Dropout(0.25))\n","    model.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\"))\n","    model.add(BatchNormalization(momentum=0.8))\n","    model.add(LeakyReLU(alpha=0.2))\n","    model.add(Dropout(0.25))\n","    model.add(Flatten())\n","    model.add(Dense(1, activation='sigmoid'))\n","\n","    img = Input(shape=self.img_shape)\n","    validity = model(img)\n","\n","    return Model(img, validity)\n","\n","  def train(self, epochs, batch_size=128, sample_interval=50):\n","    if False:\n","      print('Generator Model:')\n","      self.discriminator.summary()\n","      print('Discriminator Model:')\n","      self.combined.summary()\n","\n","    # Load the dataset\n","    (X_train, _), (_, _) = mnist.load_data()\n","    \n","    # Rescale -1 to 1\n","    X_train = X_train / 127.5 - 1.\n","    X_train = np.expand_dims(X_train, axis=3)\n","\n","    # Adversarial ground truths\n","    valid = np.ones((batch_size, 1))\n","    fake = np.zeros((batch_size, 1))\n","\n","    # Setup tensorboard\n","    # Create the TensorBoard callback,\n","    # which we will drive manually\n","    tensorboard_d = TensorBoard(\n","      log_dir= f'{self.run_tensorboard_path}/D',\n","      histogram_freq=0,\n","      batch_size=batch_size,\n","      write_graph=True,\n","      write_grads=True\n","    )\n","    \n","    tensorboard_g = TensorBoard(\n","      log_dir= f'{self.run_tensorboard_path}/G',\n","      histogram_freq=0,\n","      batch_size=batch_size,\n","      write_graph=True,\n","      write_grads=True\n","    )\n","    \n","    tensorboard_d.set_model(self.discriminator)\n","    tensorboard_g.set_model(self.combined)\n","    \n","    for epoch in range(epochs):\n","\n","      # ---------------------\n","      #  Train Discriminator\n","      # ---------------------\n","      \n","      # Generate a batch of new images\n","      noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n","      gen_imgs = self.generator.predict(noise)\n","      \n","      # Select a random batch of images\n","      idx = np.random.randint(0, X_train.shape[0], batch_size)\n","      imgs = X_train[idx]\n","      \n","\n","      # Train the discriminator\n","      d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n","      d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n","      d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n","      \n","      # ---------------------\n","      #  Train Generator\n","      # ---------------------\n","      \n","      noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n","\n","      # Train the generator (to have the discriminator label samples as valid)\n","      g_loss = self.combined.train_on_batch(noise, valid)\n","      \n","      # Save the progress\n","      # print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f, acc.: %.2f%]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss[0], g_loss[1]))\n","      tensorboard_d.on_epoch_end(epoch, self.named_logs(self.discriminator, d_loss))\n","      tensorboard_g.on_epoch_end(epoch, self.named_logs(self.combined, g_loss))\n","      \n","      # If at save interval => save generated image samples\n","      if epoch % sample_interval == 0:\n","        print(f'epoch: {epoch}')\n","        self.sample_images(epoch)\n","      \n","    tensorboard_d.on_train_end(None)\n","    tensorboard_g.on_train_end(None)\n","\n","\n","  def sample_images(self, epoch):\n","    r, c = 5, 5\n","    noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n","    gen_imgs = self.generator.predict(noise)\n","\n","    # Rescale images 0 - 1\n","    gen_imgs = 0.5 * gen_imgs + 0.5\n","\n","    fig, axs = plt.subplots(r, c)\n","    cnt = 0\n","    for i in range(r):\n","      for j in range(c):\n","        axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n","        axs[i,j].axis('off')\n","        cnt += 1\n","    fig.savefig(self.image_path + f'{epoch}.png')\n","    plt.close()\n","  \n","  # Transform train_on_batch return value\n","  # to dict expected by on_batch_end or on_epoch_end callback\n","  def named_logs(self, model, logs):\n","    result = {}\n","    for metric_name, log_value in zip(model.metrics_names, logs):\n","      result[metric_name] = log_value\n","    return result"],"execution_count":0,"outputs":[]},{"metadata":{"id":"R6HaytFTtURe","colab_type":"text"},"cell_type":"markdown","source":["### CycleGAN"]},{"metadata":{"id":"iqSALARetUBT","colab_type":"code","colab":{}},"cell_type":"code","source":["class CycleGAN(AbstractGAN):\n","  def start(self):\n","    # TODO: Read dimensions from the files.\n","    self.img_rows = 128\n","    self.img_cols = 128\n","    self.channels = 3\n","    self.img_shape = (self.img_rows, self.img_cols, self.channels)\n","    self.print_summary = True\n","\n","    optimizer_d = Adam(lr=0.0002, beta_1=0.5, beta_2=0.999, epsilon=None, decay=0, amsgrad=False)\n","    optimizer_g = Adam(lr=0.0002, beta_1=0.5, beta_2=0.999, epsilon=None, decay=0, amsgrad=False)\n","    \n","    # Number of filters in the first layer of G and D\n","    self.gf = 32\n","    self.df = 64\n","    \n","    # Loss weights\n","    self.lambda_cycle = 10.0                    # Cycle-consistency loss\n","    self.lambda_id = 0.1 * self.lambda_cycle    # Identity loss\n","    \n","    # Configure data loader\n","    self.data_loader = DataLoader(dataset_name=self.dataset_name,\n","                                  img_res=(self.img_rows, self.img_cols))\n","    \n","    \n","    # Calculate output shape of D (PatchGAN)\n","    patch = int(self.img_rows / 2**4)\n","    self.disc_patch = (patch, patch, 1)\n","\n","    # Build and compile the discriminator\n","    self.d_A = self.build_discriminator(name = 'd_A')\n","    self.d_B = self.build_discriminator(name = 'd_B')\n","    self.d_A.compile(loss={'d_A_validity':'mse'},\n","        optimizer=optimizer_d,\n","        metrics=['accuracy'])\n","    self.d_B.compile(loss={'d_B_validity':'mse'},\n","        optimizer=optimizer_d,\n","        metrics=['accuracy'])\n","    \n","    #-------------------------\n","    # Construct Computational\n","    #   Graph of Generators\n","    #-------------------------\n","\n","    # Build the generators\n","    self.g_AB = self.build_generator(name = 'g_AB')\n","    self.g_BA = self.build_generator(name = 'g_BA')\n","\n","    # Input images from both domains\n","    img_A = Input(shape=self.img_shape)\n","    img_B = Input(shape=self.img_shape)\n","\n","    # Translate images to the other domain\n","    fake_B = self.g_AB(img_A)\n","    fake_A = self.g_BA(img_B)\n","    # Translate images back to original domain\n","    reconstr_A = self.g_BA(fake_B)\n","    reconstr_B = self.g_AB(fake_A)\n","    # Identity mapping of images\n","    img_A_id = self.g_BA(img_A)\n","    img_B_id = self.g_AB(img_B)\n","\n","    # For the combined model we will only train the generators\n","    # For preveting the warning error we should create new model and freeze that one\n","    self.d_A.trainable = False\n","    self.d_B.trainable = False\n","\n","    # Discriminators determines validity of translated images\n","    valid_A = self.d_A(fake_A)\n","    valid_B = self.d_B(fake_B)\n","\n","    # Combined model trains generators to fool discriminators\n","    self.combined = Model(inputs=[img_A, img_B],\n","                outputs=[ valid_A, valid_B,\n","                    reconstr_A, reconstr_B,\n","                    img_A_id, img_B_id ], name='combined')\n","    self.combined.compile(loss=['mse', 'mse',\n","                                'mae', 'mae',\n","                                'mae', 'mae'],\n","                            loss_weights=[  1, 1,\n","                                    self.lambda_cycle, self.lambda_cycle,\n","                                    self.lambda_id, self.lambda_id ],\n","                            optimizer=optimizer_g,\n","                            metrics=['accuracy']\n","                         )\n","     \n","\n","      \n","  def build_generator(self, name = ''):\n","    \"\"\"U-Net Generator\"\"\"\n","\n","    def conv2d(layer_input, filters, f_size=4):\n","      \"\"\"Layers used during downsampling\"\"\"\n","      d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n","      d = LeakyReLU(alpha=0.2)(d)\n","      d = InstanceNormalization()(d)\n","      return d\n","\n","    def deconv2d(layer_input, skip_input, filters, f_size=4, dropout_rate=0):\n","      \"\"\"Layers used during upsampling\"\"\"\n","      u = UpSampling2D(size=2)(layer_input)\n","      u = Conv2D(filters, kernel_size=f_size, strides=1, padding='same', activation='relu')(u)\n","      if dropout_rate:\n","        u = Dropout(dropout_rate)(u)\n","      u = InstanceNormalization()(u)\n","      u = Concatenate()([u, skip_input])\n","      return u\n","\n","    # Image input\n","    d0 = Input(shape=self.img_shape)\n","\n","    # Downsampling\n","    d1 = conv2d(d0, self.gf)\n","    d2 = conv2d(d1, self.gf*2)\n","    d3 = conv2d(d2, self.gf*4)\n","    d4 = conv2d(d3, self.gf*8)\n","\n","    # Upsampling\n","    u1 = deconv2d(d4, d3, self.gf*4)\n","    u2 = deconv2d(u1, d2, self.gf*2)\n","    u3 = deconv2d(u2, d1, self.gf)\n","\n","    u4 = UpSampling2D(size=2)(u3)\n","    output_img = Conv2D(self.channels, kernel_size=4, strides=1, padding='same', activation='tanh', name= name + '_output_img')(u4)\n","\n","    return Model(d0, output_img, name=name)\n","\n","  def build_discriminator(self, name = ''):\n","\n","    def d_layer(layer_input, filters, f_size=4, normalization=True):\n","      \"\"\"Discriminator layer\"\"\"\n","      d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n","      d = LeakyReLU(alpha=0.2)(d)\n","      if normalization:\n","        d = InstanceNormalization()(d)\n","      return d\n","\n","    img = Input(shape=self.img_shape)\n","\n","    d1 = d_layer(img, self.df, normalization=False)\n","    d2 = d_layer(d1, self.df*2)\n","    d3 = d_layer(d2, self.df*4)\n","    d4 = d_layer(d3, self.df*8)\n","\n","    validity = Conv2D(1, kernel_size=4, strides=1, padding='same', name=name + '_validity')(d4)\n","\n","    return Model(img, validity, name=name)\n","\n","  def train(self, epochs, batch_size=1, sample_interval=50):\n","    self.batch_size = batch_size\n","    if self.print_summary:\n","      print('Discriminator Model:')\n","      self.d_A.summary()\n","      self.d_A.summary()\n","      print('Generators Model:')\n","      self.combined.summary()\n","\n","    # Adversarial loss ground truths\n","    valid = np.ones((self.batch_size,) + self.disc_patch)\n","    fake = np.zeros((self.batch_size,) + self.disc_patch)\n","\n","    # Setup tensorboard\n","    tensorboard_d_A = self.init_tensorboard(self.d_A, 'D_A')\n","    tensorboard_d_B = self.init_tensorboard(self.d_B, 'D_B')\n","    tensorboard_g = self.init_tensorboard(self.combined, 'G')\n","    \n","    for epoch in range(epochs):\n","      for batch_i, (imgs_A, imgs_B) in enumerate(self.data_loader.load_batch(self.batch_size)):\n","        # ----------------------\n","        #  Train Discriminators\n","        # ----------------------\n","\n","        # Translate images to opposite domain\n","        fake_B = self.g_AB.predict(imgs_A)\n","        fake_A = self.g_BA.predict(imgs_B)\n","\n","        # Train the discriminators (original images = real / translated = Fake)\n","        dA_loss_real = self.d_A.train_on_batch(imgs_A, valid)\n","        dA_loss_fake = self.d_A.train_on_batch(fake_A, fake)\n","        dA_loss = 0.5 * np.add(dA_loss_real, dA_loss_fake)\n","\n","        dB_loss_real = self.d_B.train_on_batch(imgs_B, valid)\n","        dB_loss_fake = self.d_B.train_on_batch(fake_B, fake)\n","        dB_loss = 0.5 * np.add(dB_loss_real, dB_loss_fake)\n","\n","        # Total disciminator loss\n","        d_loss = 0.5 * np.add(dA_loss, dB_loss)\n","\n","\n","        # ------------------\n","        #  Train Generators\n","        # ------------------\n","\n","        # Train the generators\n","        g_loss = self.combined.train_on_batch([imgs_A, imgs_B],\n","                            [valid, valid,\n","                            imgs_A, imgs_B,\n","                            imgs_A, imgs_B])\n","\n","        # Plot the progress\n","#         print (\"[Epoch %d/%d] [Batch %d/%d] [D loss: %f, acc: %3d%%] [G loss: %05f, adv: %05f, recon: %05f, id: %05f] time: %s \" \\\n","#                                     % ( epoch, epochs,\n","#                                       batch_i, self.data_loader.n_batches,\n","#                                       d_loss[0], 100*d_loss[1],\n","#                                       g_loss[0],\n","#                                       np.mean(g_loss[1:3]),\n","#                                       np.mean(g_loss[3:5]),\n","#                                       np.mean(g_loss[5:6]),\n","#                                       elapsed_time))\n","##      g_loss = 1 * adversarial loss + 10 * Cycle-consistency loss + 1 * Identity loss\n","\n","        tensorboard_d_A.on_batch_end(None, self.named_logs(self.d_A, dA_loss))\n","        tensorboard_d_B.on_batch_end(None, self.named_logs(self.d_B, dB_loss))\n","        tensorboard_g.on_batch_end(None, self.named_logs(self.combined, g_loss))\n","        # If at save interval => save generated image samples\n","        if batch_i % sample_interval == 0:\n","          print(f'epoch: {epoch}, batch: {batch_i}')\n","          self.sample_images(epoch, batch_i)\n","      print(f'epoch: {epoch} done.')\n","      self.copy_to_gdrive()\n","    tensorboard_d_A.on_train_end(None)\n","    tensorboard_d_B.on_train_end(None)\n","    tensorboard_g.on_train_end(None)\n","\n","  def sample_images(self, epoch, batch_i):\n","    r, c = 2, 3\n","\n","    imgs_A = self.data_loader.load_data(domain=\"A\", batch_size=1, is_testing=True)\n","    imgs_B = self.data_loader.load_data(domain=\"B\", batch_size=1, is_testing=True)\n","\n","    # Demo (for GIF)\n","    #imgs_A = self.data_loader.load_img('datasets/apple2orange/testA/n07740461_1541.jpg')\n","    #imgs_B = self.data_loader.load_img('datasets/apple2orange/testB/n07749192_4241.jpg')\n","\n","    # Translate images to the other domain\n","    fake_B = self.g_AB.predict(imgs_A)\n","    fake_A = self.g_BA.predict(imgs_B)\n","    # Translate back to original domain\n","    reconstr_A = self.g_BA.predict(fake_B)\n","    reconstr_B = self.g_AB.predict(fake_A)\n","\n","    gen_imgs = np.concatenate([imgs_A, fake_B, reconstr_A, imgs_B, fake_A, reconstr_B])\n","\n","    # Rescale images 0 - 1\n","    gen_imgs = 0.5 * gen_imgs + 0.5\n","\n","    titles = ['Original', 'Translated', 'Reconstructed']\n","    fig, axs = plt.subplots(r, c)\n","    cnt = 0\n","    for i in range(r):\n","      for j in range(c):\n","        axs[i,j].imshow(gen_imgs[cnt])\n","        axs[i, j].set_title(titles[j])\n","        axs[i,j].axis('off')\n","        cnt += 1\n","    fig.savefig(\"%s/%d_%d.png\" % (self.run_image_path, epoch, batch_i))\n","    plt.close()    \n","\n","  \n","  "],"execution_count":0,"outputs":[]},{"metadata":{"id":"wiVzINXGVwFr","colab_type":"text"},"cell_type":"markdown","source":["### Utilities"]},{"metadata":{"id":"wocxlCO2V3Vh","colab_type":"code","colab":{}},"cell_type":"code","source":["import imageio\n","from skimage.transform import resize\n","from glob import glob\n","import numpy as np\n","from tensorflow.python.keras.utils.data_utils import get_file\n","\n","class DataLoader():\n","  def __init__(self, dataset_name, img_res=(128, 128), GAN_type = 'cycleGAN', download_url = ''):\n","    self.dataset_name = dataset_name\n","    self.download_url = download_url\n","    self.img_res = img_res\n","    self.GAN_type = GAN_type\n","    self.download_dataset()\n","\n","  def download_dataset(self):\n","    \n","    if self.GAN_type == 'cycleGAN':\n","      url = f'https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/{self.dataset_name}.zip'\n","    else:\n","      url = download_url\n","    \n","    print(f'Downloading the dataset \"{self.dataset_name}\"')\n","    \n","    path = get_file(self.dataset_name + '.zip',\n","                    origin= url,\n","                    cache_dir='/content/',\n","                    extract = True\n","                   )\n","    print(f'Dataset \"{self.dataset_name}\" downloaded and extracted in \"{path}\"') \n","    \n","  \n","  def load_data(self, domain, batch_size=1, is_testing=False):\n","    data_type = \"train%s\" % domain if not is_testing else \"test%s\" % domain\n","    path = glob('./datasets/%s/%s/*' % (self.dataset_name, data_type))\n","    batch_images = np.random.choice(path, size=batch_size)\n","\n","    imgs = []\n","    for img_path in batch_images:\n","      img = self.imread(img_path)\n","      if not is_testing:\n","        img = resize(img, self.img_res, mode = 'reflect')\n","\n","        if np.random.random() > 0.5:\n","            img = np.fliplr(img)\n","      else:\n","        img = resize(img, self.img_res, mode = 'reflect')\n","      imgs.append(img)\n","\n","    imgs = np.array(imgs)/127.5 - 1.\n","\n","    return imgs\n","\n","  def load_batch(self, batch_size=1, is_testing=False):\n","    data_type = \"train\" if not is_testing else \"val\"\n","    path_A = glob('./datasets/%s/%sA/*' % (self.dataset_name, data_type))\n","    path_B = glob('./datasets/%s/%sB/*' % (self.dataset_name, data_type))\n","    self.n_batches = int(min(len(path_A), len(path_B)) / batch_size)\n","    total_samples = self.n_batches * batch_size\n","\n","    # Sample n_batches * batch_size from each path list so that model sees all\n","    # samples from both domains\n","    path_A = np.random.choice(path_A, total_samples, replace=False)\n","    path_B = np.random.choice(path_B, total_samples, replace=False)\n","\n","    for i in range(self.n_batches-1):\n","      batch_A = path_A[i*batch_size:(i+1)*batch_size]\n","      batch_B = path_B[i*batch_size:(i+1)*batch_size]\n","      imgs_A, imgs_B = [], []\n","      for img_A, img_B in zip(batch_A, batch_B):\n","        img_A = self.imread(img_A)\n","        img_B = self.imread(img_B)\n","\n","        img_A = resize(img_A, self.img_res, mode = 'reflect')\n","        img_B = resize(img_B, self.img_res, mode = 'reflect')\n","\n","        if not is_testing and np.random.random() > 0.5:\n","                img_A = np.fliplr(img_A)\n","                img_B = np.fliplr(img_B)\n","\n","        imgs_A.append(img_A)\n","        imgs_B.append(img_B)\n","\n","      imgs_A = np.array(imgs_A)/127.5 - 1.\n","      imgs_B = np.array(imgs_B)/127.5 - 1.\n","\n","      yield imgs_A, imgs_B\n","\n","  def load_img(self, path):\n","    img = self.imread(path)\n","    img = resize(img, self.img_res, mode = 'reflect')\n","    img = img/127.5 - 1.\n","    return img[np.newaxis, :, :, :]\n","\n","  def imread(self, path):\n","    return imageio.imread(path, pilmode='RGB').astype(np.float)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"sFeqfclftUuI","colab_type":"text"},"cell_type":"markdown","source":["## RUN"]},{"metadata":{"id":"RCYNQtCojKn3","colab_type":"text"},"cell_type":"markdown","source":["#### Configs"]},{"metadata":{"id":"4y2k9SgCzfWG","colab_type":"code","cellView":"form","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"8ad40ca4-d000-471b-fa39-cc5ac2715adc","executionInfo":{"status":"ok","timestamp":1548736142413,"user_tz":300,"elapsed":55064,"user":{"displayName":"Ali Panahi","photoUrl":"https://lh5.googleusercontent.com/-aAEVDWwWDxU/AAAAAAAAAAI/AAAAAAAAEuM/WOpI1BZp8tM/s64/photo.jpg","userId":"09333706783536538061"}}},"cell_type":"code","source":["#@title General Configurations  { run: \"auto\" }\n","if __name__ == '__main__':\n","  # User Defined Variables\n","  Enable_Google_Drive = False #@param {type:\"boolean\"}\n","  Enable_Tensorboard = True #@param {type:\"boolean\"}\n","  Enable_Tensorboard_Debugger = False #@param {type:\"boolean\"}\n","  Reset_Tensorboard = False #@param {type:\"boolean\"}\n","  Enable_Web_PDB = False #@param {type:\"boolean\"}\n","  Tensorboard_Path = './tensorboard/' #@param {type:\"string\"}\n","  Image_Path = './images/' #@param {type:\"string\"}\n","  #@markdown You need to create a folder in your Google Drive and set the folder address in here if you plan to use Google Drive\n","  GDrive_Project_Path = '/GANs Research/' #@param {type:\"string\"}\n","    \n","  #Flag Variables\n","  # Check to see if this cell is executed before\n","  try: configs_cell_executed\n","  except NameError:\n","    google_drive_mounted = False\n","    tensorboard_installed = False\n","    tensorboard_runned = False\n","    web_pdb_installed = False\n","    web_pdb_runned = False\n","\n","  if Enable_Google_Drive is True and google_drive_mounted is False:\n","    GDrive_Project_Moundted_Path = mount_Google_Drive(GDrive_Project_Path)\n","    google_drive_mounted = True\n","\n","  if Enable_Tensorboard is True and tensorboard_installed is False:\n","    tensorboard_installer()\n","    tensorboard_installed = True\n","\n","  if Enable_Tensorboard is True and tensorboard_runned is True:\n","    print('Tensorboard: ' + tbc.tensorboard_link)\n","    \n","  if (Enable_Tensorboard is True and tensorboard_runned is False) or Reset_Tensorboard is True:\n","    tbc = tensorboard_runner(Tensorboard_Path, Enable_Tensorboard_Debugger)\n","    tensorboard_runned = True\n","\n","  if Enable_Web_PDB is True and web_pdb_installed is False:\n","    web_pdb_installer()\n","    web_pdb_installed = True\n","\n","  if Enable_Web_PDB is True and web_pdb_runned is False:\n","    web_pdb_runner()\n","    web_pdb_runned = True\n","  \n","  if Enable_Web_PDB is True:\n","    !cat web_pdb.txt\n","\n","\n","  configs_cell_executed = True"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Wait for 8 seconds...\n","TensorBoard link:\n","http://396ff29c.ngrok.io\n"],"name":"stdout"}]},{"metadata":{"id":"uimOeHpwJPWV","colab_type":"code","cellView":"form","colab":{"base_uri":"https://localhost:8080/","height":2128},"outputId":"b517c1b4-a8e0-4acc-9415-6c532dbb522e"},"cell_type":"code","source":["#@title GAN Configurations\n","if __name__ == '__main__':\n","  from distutils.dir_util import copy_tree\n","  import time\n","  import datetime\n","  import os\n","  from pytz import timezone\n","  \n","  start_time = time.time()\n","  #@markdown Which GAN network you want to use?\n","  GAN_type = \"CycleGAN\" #@param [\"CycleGAN\", \"DCGAN\", \"OriginalGAN\"]\n","  #@markdown Select the dataset that you want to use for CycleGAN.\n","  cycleGAN_dataset = \"horse2zebra\" #@param [\"ae_photos\", \"apple2orange\", \"summer2winter_yosemite\", \"horse2zebra\", \"monet2photo\", \"cezanne2photo\", \"ukiyoe2photo\", \"vangogh2photo\", \"maps\", \"cityscapes\", \"facades\", \"iphone2dslr_flower\"]\n","  #@markdown Select the dataset that you want to use for OriginalGAN or DCGAN\n","  OriginalGAN_DCGAN_dataset = \"mnist\" #@param ['boston_housing', 'cifar', 'cifar10', 'cifar100', 'fashion_mnist', 'imdb', 'mnist', 'reuters']\n","  epochs = 100 #@param {type:\"integer\"}\n","  batch_size = 1 #@param {type:\"integer\"}\n","  sample_interval = 50 #@param {type:\"integer\"}\n","  #@markdown ---\n","  #@markdown Check or Uncheck to Run:\n","  Run = False #@param {type:\"boolean\"}\n","  \n","  # RUN\n","  # Folder name\n","  folder_prefix = GAN_type\n","  folder_postfix = ''\n","  \n","  # add datetime to folder name\n","  now = datetime.datetime.now().astimezone(timezone('US/Eastern'))\n","  time_for_different_run = f'{now.year}-{now.month}-{now.day}-{now.hour}-{now.minute}-{now.second}'\n","  run_folder_name = folder_prefix + '-' + time_for_different_run + '-' + folder_postfix\n","  \n","  run_image_path = f'{Image_Path}/{run_folder_name}/'\n","  run_tensorboard_path = f'{Tensorboard_Path}/{run_folder_name}/'\n","  \n","  if Enable_Google_Drive is True:\n","    GDrive_run_image_path = f'{GDrive_Project_Moundted_Path}/{run_image_path}/'\n","    GDrive_run_tensorboard_path = f'{GDrive_Project_Moundted_Path}/{run_tensorboard_path}/'\n","    \n","  if not os.path.exists(run_image_path):\n","    os.makedirs(run_image_path)\n","   \n","  # Run the GAN Network\n","#   import web_pdb; web_pdb.set_trace()\n","  GAN_class = eval(GAN_type)\n","  if Enable_Tensorboard_Debugger is True:\n","    gan = GAN_class(run_image_path, run_tensorboard_path, cycleGAN_dataset,\n","                    TensorBoardDebugWrapperSession = \"67f39d12e309:6064\",\n","                    GDrive_run_image_path = GDrive_run_image_path,\n","                    GDrive_run_tensorboard_path = GDrive_run_tensorboard_path)\n","  else:\n","    gan = GAN_class(run_image_path, run_tensorboard_path, cycleGAN_dataset)\n","  gan.train(epochs, batch_size, sample_interval)\n","  \n","  # Done\n","  print(\"--- %s seconds ---\" % (time.time() - start_time))\n","  "],"execution_count":0,"outputs":[{"output_type":"stream","text":["Downloading the dataset \"horse2zebra\"\n","Downloading data from https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/horse2zebra.zip\n","116875264/116867962 [==============================] - 43s 0us/step\n","Dataset \"horse2zebra\" downloaded and extracted in \"/content/datasets/horse2zebra.zip\"\n","Discriminator Model:\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_1 (InputLayer)         (None, 128, 128, 3)       0         \n","_________________________________________________________________\n","conv2d_1 (Conv2D)            (None, 64, 64, 64)        3136      \n","_________________________________________________________________\n","leaky_re_lu_1 (LeakyReLU)    (None, 64, 64, 64)        0         \n","_________________________________________________________________\n","conv2d_2 (Conv2D)            (None, 32, 32, 128)       131200    \n","_________________________________________________________________\n","leaky_re_lu_2 (LeakyReLU)    (None, 32, 32, 128)       0         \n","_________________________________________________________________\n","instance_normalization_1 (In (None, 32, 32, 128)       2         \n","_________________________________________________________________\n","conv2d_3 (Conv2D)            (None, 16, 16, 256)       524544    \n","_________________________________________________________________\n","leaky_re_lu_3 (LeakyReLU)    (None, 16, 16, 256)       0         \n","_________________________________________________________________\n","instance_normalization_2 (In (None, 16, 16, 256)       2         \n","_________________________________________________________________\n","conv2d_4 (Conv2D)            (None, 8, 8, 512)         2097664   \n","_________________________________________________________________\n","leaky_re_lu_4 (LeakyReLU)    (None, 8, 8, 512)         0         \n","_________________________________________________________________\n","instance_normalization_3 (In (None, 8, 8, 512)         2         \n","_________________________________________________________________\n","d_A_validity (Conv2D)        (None, 8, 8, 1)           8193      \n","=================================================================\n","Total params: 5,529,486\n","Trainable params: 2,764,743\n","Non-trainable params: 2,764,743\n","_________________________________________________________________\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_1 (InputLayer)         (None, 128, 128, 3)       0         \n","_________________________________________________________________\n","conv2d_1 (Conv2D)            (None, 64, 64, 64)        3136      \n","_________________________________________________________________\n","leaky_re_lu_1 (LeakyReLU)    (None, 64, 64, 64)        0         \n","_________________________________________________________________\n","conv2d_2 (Conv2D)            (None, 32, 32, 128)       131200    \n","_________________________________________________________________\n","leaky_re_lu_2 (LeakyReLU)    (None, 32, 32, 128)       0         \n","_________________________________________________________________\n","instance_normalization_1 (In (None, 32, 32, 128)       2         \n","_________________________________________________________________\n","conv2d_3 (Conv2D)            (None, 16, 16, 256)       524544    \n","_________________________________________________________________\n","leaky_re_lu_3 (LeakyReLU)    (None, 16, 16, 256)       0         \n","_________________________________________________________________\n","instance_normalization_2 (In (None, 16, 16, 256)       2         \n","_________________________________________________________________\n","conv2d_4 (Conv2D)            (None, 8, 8, 512)         2097664   \n","_________________________________________________________________\n","leaky_re_lu_4 (LeakyReLU)    (None, 8, 8, 512)         0         \n","_________________________________________________________________\n","instance_normalization_3 (In (None, 8, 8, 512)         2         \n","_________________________________________________________________\n","d_A_validity (Conv2D)        (None, 8, 8, 1)           8193      \n","=================================================================\n","Total params: 5,529,486\n","Trainable params: 2,764,743\n","Non-trainable params: 2,764,743\n","_________________________________________________________________\n","Generators Model:\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_6 (InputLayer)            (None, 128, 128, 3)  0                                            \n","__________________________________________________________________________________________________\n","input_5 (InputLayer)            (None, 128, 128, 3)  0                                            \n","__________________________________________________________________________________________________\n","g_BA (Model)                    (None, 128, 128, 3)  1545425     input_6[0][0]                    \n","                                                                 g_AB[1][0]                       \n","                                                                 input_5[0][0]                    \n","__________________________________________________________________________________________________\n","g_AB (Model)                    (None, 128, 128, 3)  1545425     input_5[0][0]                    \n","                                                                 g_BA[1][0]                       \n","                                                                 input_6[0][0]                    \n","__________________________________________________________________________________________________\n","d_A (Model)                     (None, 8, 8, 1)      2764743     g_BA[1][0]                       \n","__________________________________________________________________________________________________\n","d_B (Model)                     (None, 8, 8, 1)      2764743     g_AB[1][0]                       \n","==================================================================================================\n","Total params: 8,620,336\n","Trainable params: 3,090,850\n","Non-trainable params: 5,529,486\n","__________________________________________________________________________________________________\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n","  'Discrepancy between trainable weights and collected trainable'\n","/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n","  'Discrepancy between trainable weights and collected trainable'\n"],"name":"stderr"},{"output_type":"stream","text":["epoch: 0, batch: 0\n","epoch: 0, batch: 50\n","epoch: 0, batch: 100\n","epoch: 0, batch: 150\n","epoch: 0, batch: 200\n","epoch: 0, batch: 250\n","epoch: 0, batch: 300\n","epoch: 0, batch: 350\n","epoch: 0, batch: 400\n","epoch: 0, batch: 450\n","epoch: 0, batch: 500\n","epoch: 0, batch: 550\n","epoch: 0, batch: 600\n","epoch: 0, batch: 650\n","epoch: 0, batch: 700\n","epoch: 0, batch: 750\n","epoch: 0, batch: 800\n","epoch: 0, batch: 850\n","epoch: 0, batch: 900\n","epoch: 0, batch: 950\n","epoch: 0, batch: 1000\n","epoch: 0, batch: 1050\n","epoch: 0 done.\n","epoch: 1, batch: 0\n"],"name":"stdout"}]},{"metadata":{"id":"vC0zZ0qzccLE","colab_type":"text"},"cell_type":"markdown","source":["## To Do\n","- Cache Datasets in google drive and then copy to /content\n","- Test with Binary Cross Entropy and Hinge loss and compare the results\n","- Test with map dataset"]}]}